DPDK组件：
2.1、 环形缓冲区管理(librte_ring)
Ring数据结构提供了一个无锁的多生产者，多消费者的FIFO表处理接口。 他比无锁队列优异的地方在于它容易部署，适合大量的操作，而且更快。 Ring库在 Memory Pool Manager (librte_mempool) 中使用到， 而且ring还用于不同核之间或是逻辑核上处理单元之间的通信。 Ring缓存机制及其使用可以参考 Ring Library。

2.2. 内存池管理(librte_mempool)
内存池管理的主要职责就是在内存中分配指定数目对象的POOL。 每个POOL以名称来唯一标识，并且使用一个ring来存储空闲的对象节点。 它还提供了一些其他的服务如对象节点的每核备份缓存及自动对齐以保证元素能均衡的处于每核内存通道上。 内存池分配器具体行为参考 Mempool Library。

2.3. 网络报文缓冲区管理(librte_mbuf)
报文缓存管理器提供了创建、释放报文缓存的能力，DPDK应用程序中可能使用这些报文缓存来存储消息。 而消息通常在程序开始时通过DPDK的MEMPOOL库创建并存储。 BUFF库提供了报文申请释放的API，通常消息buff用于缓存普通消息，报文buff用于缓存网络报文。 报文缓存管理参考 Mbuf Library。

2.4. 定时器管理(librte_timer)
这个库位DPDK执行单元提供了定时服务，为函数异步执行提供支持。 定时器可以设置周期调用或只调用一次。 使用EAL提供的接口获取高精度时钟，并且能在每个核上根据需要初始化。 具体参考 Timer Library。

3、以太网轮询驱动架构
DPDK的PMD驱动支持1G、10G、40G。 同时DPDK提供了虚拟的以太网控制器，被设计成非异步，基于中断的模式。 详细内容参考 Poll Mode Driver。

4、 报文转发算法支持
DPDK提供了哈希（librte_hash）、最长前缀匹配的（librte_lpm）算法库用于支持包转发。 详细内容查看 Hash Library 和 LPM Library 。

5、网络协议库(librte_net)
这个库提供了IP协议的一些定义，以及一些常用的宏。 这些定义都基于FreeBSD IP协议栈的代码，并且包含相关的协议号，IP相关宏定义，IPV4和IPV6头部结构等等
http://blog.csdn.net/cling60/article/details/77871280

==============================================================================
基于 OS 内核的数据传输有什么弊端？
1、中断处理。当网络中大量数据包到来时，会产生频繁的硬件中断请求，这些硬件中断可以打断之前较低优先级的软中断或者系统调用的执行过程，如果这种打断频繁的话，将会产生较高的性能开销。

2、内存拷贝。正常情况下，一个网络数据包从网卡到应用程序需要经过如下的过程：数据从网卡通过 DMA 等方式传到内核开辟的缓冲区，然后从内核空间拷贝到用户态空间，在 Linux 内核协议栈中，这个耗时操作甚至占到了数据包整个处理流程的 57.1%。

3、上下文切换。频繁到达的硬件中断和软中断都可能随时抢占系统调用的运行，这会产生大量的上下文切换开销。另外，在基于多线程的服务器设计框架中，线程间的调度也会产生频繁的上下文切换开销，同样，锁竞争的耗能也是一个非常严重的问题。

4、局部性失效。如今主流的处理器都是多个核心的，这意味着一个数据包的处理可能跨多个 CPU 核心，比如一个数据包可能中断在 cpu0，内核态处理在 cpu1，用户态处理在 cpu2，这样跨多个核心，容易造成 CPU 缓存失效，造成局部性失效。如果是 NUMA 架构，更会造成跨 NUMA 访问内存，性能受到很大影响。

5、内存管理。传统服务器内存页为 4K，为了提高内存的访问速度，避免 cache miss，可以增加 cache 中映射表的条目，但这又会影响 CPU 的检索效率。

综合以上问题，可以看出内核本身就是一个非常大的瓶颈所在。那很明显解决方案就是想办法绕过内核。

==============================================================================

传统 Linux 内核网络数据流程：
硬件中断--->取包分发至内核线程--->软件中断--->内核线程在协议栈中处理包--->处理完毕通知用户层
用户层收包-->网络层--->逻辑层--->业务层

Linux内核NAPI机制：
系统被中断唤醒后，尽量使用轮询的方式一次处理多个数据包，知道网络再次空闲重新转入中断等待。NAPI适用于高吞吐的网络场景，可以显著提升效率。


dpdk 网络数据流程：
硬件中断--->放弃中断流程
用户层通过设备映射取包--->进入用户层协议栈--->逻辑层--->业务层

DPDK技术特点：
轮询：避免上下文切换的开销
用户态驱动：避免不必要的拷贝和系统调用
亲和性与独占：利用线程亲和绑定的方式，特定任务被指定到只在某个核上工作，同时可以隔离部分核使其不参与Linux的系统调度，使线程独占核，保证更多的cache hit，同时也避免同一个核的多任务开销切换。
降低存仿开销：网络数据包的处理是一种典型的IO密集型工作场景，使用大页内存能降低TLB miss，利用多通道的交错访问能够有效提高内存访问的有效带宽。
软件调优：结构的cache line对齐，多核间避免跨cache line共享

==============================================================================

Linux下CPU隔离：
在grub.conf上使用”isolcpus=[num1]，[num2]... “参数，进行cpu的隔离，使得系统启动后普通进程默认不会调度到被隔离的cpu上执行。（注：num表示逻辑cpu号码）
例子：
	title CentOS (3.2.84)
    root (hd0,0)        kernel /vmlinuz-3.2.84 ro root=/dev/mapper/VolGroup-lv_root isolcups=4,5 rd_NO_LUKS rd_NO_MD rd_LVM_LV=VolGroup/lv_swap crashkernel=auto LANG=en rd_LVM_LV=VolGroup/lv_root  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet 

initrd /initramfs-3.2.84.img
注：isolcups要紧跟在root后面，放在最后不行

查看是否隔离成功：
[root@localhost ~]# ps -eLo psr,args|awk '{if ($1==3) print $0}'
ps（process status）：
	-e:显示所有进程
	-L:用于将线程（LWP，lightweight process）也显示出来，（这里我忽略这个结果了）
	-o：以用户自定义格式输出（可选参数有psr、pid、ppid等）
	-o参数里面： 
		psr：当前分配给进程运行的处理器编号
		args：表示运行进程的命令和参数
这里我们看到进程编号2、3都有很多进程，而4、5都只有5个带[]号的系统进程，所以隔离成功。
==============================================================================
Linux下的CPU亲和性：
内核中，所有线程都有一个相关的数据结构task_struct
其中与CPU亲和性相关度最高的是cpus_allowed位掩码，这个掩码位由n位组成，与系统中n个逻辑处理器对应

Linux内核中提供的设置亲和性的方法：
sched_set_affinity()
sched_get_affinity()
将线程与CPU绑定，最直观的好处就是提高了CPU Cache的命中率，减少内存访问损耗，提高速度。

==============================================================================
DPDK提供的库：
Core Libs核心库：提供系统抽象，大页内存，缓存池，定时器，无锁队列等
PMD库：提供用户态驱动，以便通过轮询和线程绑定得到极高的网络吞吐，支持较多型号的物理网卡和虚拟网卡
Classify库：支持精确匹配，最长匹配，通配符匹配，提供常用包处理的查表操作
QoS库：提供服务质量相关组件，如限速Meter，调度Sched
KNI：Kernel Network Interface，内核协议栈快速通道
==============================================================================

NUMA架构：Non-Uniform Memory Architecture
每个处理器有自己的本地内存，每个处理器访问本地内存时耗时较短，访问远程内存时，需要跨总线，耗时较长

==============================================================================

内存种类：
RAM： 随机访问存储器
SRAM：静态随机访问存储器
DRAM：动态随机访问存储器

Cache是一种SRAM，Cache分为3级：
一级Cache：分为指令Cache和数据Cache，一级Cache一般处理周期为3~5个周期，一般只有几十kB，多核处理器每个核心都有自己的Cache
二级Cache：数据和指令无差别的存储在一起，处理周期需要十几个指令周期，一般几百kB到几MB不等，每个处理器有自己的核心
三级Cache：处理周期需要几十个指令周期，容量更大几MB到几十MB不等，三级Cache由所有核心共享

软件使用虚拟地址访问内存，处理器负责将虚拟地址映射到物理地址，为了完成工作，处理器采用多级页表进行多次查找，最终访问物理地址；
页表存储在内存中，处理器可以利用三级Cache来缓存页表，但是这样会导致处理器访问内存频率过高
TLB Cache，Translation Look-side Cache 专门用于缓存内存中的页表项，TLB查找大大减小了处理器的开销
对于占用内存大的程序，TLB不命中会逐渐增多，可以采用大分页来减少TLB不命中的情况。

Linux操作系统采用了基于hugetlbfs的特殊文件系统来加入对2MB或1GB的大页面的支持。
==============================================================================
dpdk 能够绕过内核协议栈，本质上是得益于 UIO 技术，通过 UIO 能够拦截中断，并重设中断回调行为，从而绕过内核协议栈后续的处理流程。

UIO 设备的实现机制其实是对用户空间暴露文件接口，比如当注册一个 UIO 设备 uioX，就会出现文件 /dev/uioX，对该文件的读写就是对设备内存的读写。除此之外，对设备的控制还可以通过 /sys/class/uio 下的各个文件的读写来完成。

==============================================================================

内存池技术

dpdk 在用户空间实现了一套精巧的内存池技术，内核空间和用户空间的内存交互不进行拷贝，只做控制权转移。这样，当收发数据包时，就减少了内存拷贝的开销。

内存池的创建使用的接口是rte_mempool_create()

1.mempool头结构。mempool由名字区分，挂接在struct rte_tailq_elem rte_mempool_tailq全局队列中，可以根据mempool的名字进行查找，使用rte_mempool_lookup()接口即可。这只是个mempool的指示结构，mempool分配的内存区并不在这里面，只是通过物理和虚拟地址指向实际的内存地址。

2.mempool的实际空间。这就是通过内存分配出来的地址连续的空间，用来存储mempool的obj对象。



==============================================================================

大页内存管理

dpdk 实现了一组大页内存分配、使用和释放的 API，上层应用可以很方便使用 API 申请使用大页内存，同时也兼容普通的内存申请。

==============================================================================

无锁环形队列

dpdk 基于 Linux 内核的无锁环形缓冲 kfifo 实现了自己的一套无锁机制。支持单生产者入列/单消费者出列和多生产者入列/多消费者出列操作，在数据传输的时候，降低性能的同时还能保证数据的同步.

==============================================================================

poll-mode网卡驱动

DPDK网卡驱动完全抛弃中断模式，基于轮询方式收包，避免了中断开销。

==============================================================================

NUMA

dpdk 内存分配上通过 proc 提供的内存信息，使 CPU 核心尽量使用靠近其所在节点的内存，避免了跨 NUMA 节点远程访问内存的性能问题。

==============================================================================

CPU 亲和性

dpdk 利用 CPU 的亲和性将一个线程或多个线程绑定到一个或多个 CPU 上，这样在线程执行过程中，就不会被随意调度，一方面减少了线程间的频繁切换带来的开销，另一方面避免了 CPU 缓存的局部失效性，增加了 CPU 缓存的命中率。

==============================================================================

用面向对象的思想来理解无锁队列ring。
dpdk的无锁队列ring是借鉴了linux内核kfifo无锁队列。
ring的实质是FIFO的环形队列。

ring的特点：
无锁出入队（除了cas(compare and swap)操作）
多消费/生产者同时出入队

1）无锁队列主要是通过CAS、FAA这些原子操作，和Retry-Loop实现。
2）对于Retry-Loop，我个人感觉其实和锁什么什么两样。只是这种“锁”的粒度变小了，主要是“锁”HEAD和TAIL这两个关键资源。而不是整个数据结构。

==============================================================================
性能高在哪？
DPDK并没有做socket层的协议处理，避免了从内核态往用户态的拷贝
收包时使用UIO技术在用户态驱动网卡，使用轮询收包，避免使用中断收包带来的切换开销
网卡收完包后，通过DMA直接将数据包从网卡队列拷贝到内存
通过核绑定，避免线程切换的开销
使用大页内存，减少cache hit丢失率
==============================================================================

DMA技术
1、首先，内核在主内存中为收发数据建立一个环形的缓冲队列（通常叫DMA环形缓冲区）。
2、内核将这个缓冲区通过DMA映射，把这个队列交给网卡；
3、网卡收到数据，就直接放进这个环形缓冲区了——也就是直接放进主内存了；然后，向系统产生一个中断；
4、内核收到这个中断，就取消DMA映射，这样，内核就直接从主内存中读取数据；

==============================================================================

　　DPDK核心技术如下：

　　（1）通过UIO技术将报文拷贝到应用空间处理

　　（2）通过大页内存，降低cache miss ，提高命中率，进而cpu访问速度

　　（3）通过CPU亲和性，绑定网卡和线程到固定的core，减少cpu任务切换

　　（4）通过无锁队列，减少资源竞争
==============================================================================

struct kfifo {   
    unsigned char *buffer;    /* the buffer holding the data */   
    unsigned int size;    /* the size of the allocated buffer */   
    unsigned int in;    /* data is added at offset (in % size) */   
    unsigned int out;    /* data is extracted from off. (out % size) */   
    spinlock_t *lock;    /* protects concurrent modifications */   
};

kfifo主要提供了两个操作，__kfifo_put(入队操作)和__kfifo_get(出队操作)
buffer， 用于存放数据的缓存
size,      buffer空间的大小，在初化时，将它向上扩展成2的幂
lock,      如果使用不能保证任何时间最多只有一个读线程和写线程，需要使用该lock实施同步。
in, out,  和buffer一起构成一个循环队列。 in指向buffer中队头，而且out指向buffer中的队尾，它的结构如示图如下：
+--------------------------------------------------------------+
|           |<----------data---------->|                       | 
+--------------------------------------------------------------+ 
            ^                                       ^  
            |                                       | 
            in                                     out

kfifo_alloc 分配kfifo内存和初始化工作
if (size & (size - 1)) {   
    BUG_ON(size > 0x80000000);   
    size = roundup_pow_of_two(size);   
}   
buffer = kmalloc(size, gfp_mask);

kfifo->size的值总是在调用者传进来的size参数的基础上向2的幂扩展，这是内核一贯的做法。这样的好处不言而喻--对kfifo->size取模运算可以转化为与运算，如下：
kfifo->in % kfifo->size 可以转化为 kfifo->in & (kfifo->size – 1)

在kfifo_alloc函数中，使用size & (size – 1)来判断size 是否为2幂，如果条件为真，则表示size不是2的幂，然后调用roundup_pow_of_two将之向上扩展为2的幂。

__kfifo_put是入队操作，它先将数据放入buffer里面，最后才修改in参数；
__kfifo_get是出队操作，它先将数据从buffer中移走，最后才修改out.

