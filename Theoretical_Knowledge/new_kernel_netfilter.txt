//每个链接跟踪记录项就是一个nf_conn的结构体对象
//每个连接跟踪的结构体对象中，包含了一个数组
//struct nf_conntrack_tuple_hash tuplehash[IP_CT_DIR_MAX];
//这个数组包含两个元素，下标为0的元素表示初始方向的链接记录，下标为1的元素表示应答方向的链接记录

 74 struct nf_conn {
 75     /* Usage count in here is 1 for hash table/destruct timer, 1 per skb,
 76      * plus 1 for any connection(s) we are `master' for
 77      *
 78      * Hint, SKB address this struct and refcnt via skb->nfct and
 79      * helpers nf_conntrack_get() and nf_conntrack_put().
 80      * Helper nf_ct_put() equals nf_conntrack_put() by dec refcnt,
 81      * beware nf_ct_get() is different and don't inc refcnt.
 82      */
 83     struct nf_conntrack ct_general;
 84 
 85     spinlock_t  lock;
 86     u16     cpu;
 87 
 88     /* XXX should I move this to the tail ? - Y.K */
 89     /* These are my tuples; original and reply */
 90     struct nf_conntrack_tuple_hash tuplehash[IP_CT_DIR_MAX];
 91 
 92     /* Have we seen traffic both ways yet? (bitset) */
 93     unsigned long status;
 94 
 95     /* If we were expected by an expectation, this will be it */
 96     struct nf_conn *master;
 97 
 98     /* Timer function; drops refcnt when it goes off. */
 99     struct timer_list timeout;
100 
101 #if defined(CONFIG_NF_CONNTRACK_MARK)
102     u_int32_t mark;
103 #endif
104 
105 #ifdef CONFIG_NF_CONNTRACK_SECMARK
106     u_int32_t secmark;
107 #endif
108 
109     /* Extensions */
110     struct nf_ct_ext *ext;
111 #ifdef CONFIG_NET_NS
112     struct net *ct_net;
113 #endif
114 
115     /* Storage reserved for other modules, must be the last member */
116     union nf_conntrack_proto proto;
117 };

==============================================================================

//struct nf_conntrack_tuple_hash是对struct nf_conntrack_tuple的封装
//其中hnnode用来将nf_conntrack_tuple_hash节点链接成双向链表

117 /* Connections have two entries in the hash table: one for each way */
118 struct nf_conntrack_tuple_hash {
119     struct hlist_nulls_node hnnode;
120     struct nf_conntrack_tuple tuple;
121 };

==============================================================================
//nf_conntrack_tuple是链接跟踪中真正记录链接信息的部分
//nf_conntrack_tuple_hash是对nf_conntrack_tuple的封装

 35 /* This contains the information to distinguish a connection. */
 36 struct nf_conntrack_tuple {
 37     struct nf_conntrack_man src;
 38 
 39     /* These are the parts of the tuple which are fixed. */
 40     struct {
 41         union nf_inet_addr u3;
 42         union {
 43             /* Add other protocols here. */
 44             __be16 all;
 45 
 46             struct {
 47                 __be16 port;
 48             } tcp;
 49             struct {
 50                 __be16 port;
 51             } udp;
 52             struct {
 53                 u_int8_t type, code;
 54             } icmp;
 55             struct {
 56                 __be16 port;
 57             } dccp;
 58             struct {
 59                 __be16 port;
 60             } sctp;
 61             struct {
 62                 __be16 key;
 63             } gre;
 64         } u;
 65 
 66         /* The protocol. */
 67         u_int8_t protonum;
 68 
 69         /* The direction (for tuplehash) */
 70         u_int8_t dir;
 71     } dst;
 72 };

==============================================================================

//数据包进入链接跟踪，开始进行连接跟踪的处理
//链接跟踪的入口函数为nf_conntrack_in
//net/netfilter/nf_conntrack_core.c
unsigned int
nf_conntrack_in(struct net *net, u_int8_t pf, unsigned int hooknum,
        struct sk_buff *skb)
{
    struct nf_conn *ct, *tmpl = NULL;
    enum ip_conntrack_info ctinfo;
    struct nf_conntrack_l3proto *l3proto;
    struct nf_conntrack_l4proto *l4proto;
    unsigned int dataoff;
    u_int8_t protonum;
    int set_reply = 0;
    int ret;

     /* 
        这个条件判断不是很明白。为什么可以直接忽略以前的nfct。
        看了后面的代码后，skb->nfcth会在后面的代码中重新得到，再配合这里的注释大概明白了用意。
        当skb->nfct为有效值时，即意味着该skb已经经过了conn track，再次落到conn track时。
        如注释所说，比如发往回环的时候，会有这个情况。
     */
    if (skb->nfct) {
        /* Previously seen (loopback or untracked)? Ignore. */
        tmpl = (struct nf_conn *)skb->nfct;
        if (!nf_ct_is_template(tmpl)) {
            NF_CT_STAT_INC_ATOMIC(net, ignore);
            return NF_ACCEPT;
        }
        skb->nfct = NULL;
    }

    /* rcu_read_lock()ed by nf_hook_slow */
    /* 
        根据proto family，来得到l3的conn track的target。
        这里对于netfilter来说，它将conn track的具体工作下发到具体的L3层的target处理。
        这样无论是ipv4还是ipv6，netfilter的处理可以保持一致。甚至其可以支持更多的L3协议。
        对于ipv4来说，l3proto就是nf_conntrack_l3proto_ipv4。
    */
    l3proto = __nf_ct_l3proto_find(pf);

    /* 
        将L3的报文头的偏移即数据包skb传给l3->get_l4proto来得到L4的协议号即L4的起始位置。
        这时L3的报文头完全由具体的L3层的proto负责解析
    */
    ret = l3proto->get_l4proto(skb, skb_network_offset(skb),
                 &dataoff, &protonum);
    if (ret <= 0) {
        pr_debug("not prepared to track yet or error occured\n");
        NF_CT_STAT_INC_ATOMIC(net, error);
        NF_CT_STAT_INC_ATOMIC(net, invalid);
        ret = -ret;
        goto out;
    }
     
     /*
        与L3类似，同样是通过L4的协议号得到具体的L4 proto的target。
        实现与具体协议的解耦。
        如nf_conntrack_l4proto_tcp4， nf_conntrack_l4proto_udp4等等
     */
    l4proto = __nf_ct_l4proto_find(pf, protonum);

    /* It may be an special packet, error, unclean...
     * inverse of the return code tells to the netfilter
     * core what to do with the packet. */
    if (l4proto->error != NULL) {

        /* 对L4数据包进行检查，由具体的L4协议负责 */
        ret = l4proto->error(net, tmpl, skb, dataoff, &ctinfo,
                 pf, hooknum);
        if (ret <= 0) {
            NF_CT_STAT_INC_ATOMIC(net, error);
            NF_CT_STAT_INC_ATOMIC(net, invalid);
            ret = -ret;
            goto out;
        }
    }
    
     /* 
        根据L3和L4层的信息，得到conn track和其信息。后面会学习resolve_normal_ct 
     */
    ct = resolve_normal_ct(net, tmpl, skb, dataoff, pf, protonum,
             l3proto, l4proto, &set_reply, &ctinfo);
    
    if (!ct) {
        
        /* Not valid part of a connection */
        NF_CT_STAT_INC_ATOMIC(net, invalid);
        ret = NF_ACCEPT;
        goto out;
    }

    if (IS_ERR(ct)) {
        /* Too stressed to deal. */
        NF_CT_STAT_INC_ATOMIC(net, drop);
        ret = NF_DROP;
        goto out;
    }

    NF_CT_ASSERT(skb->nfct);
     
     /* 
        将数据包传递给具体的L4 target进行特定的操作。
        如nf_conntrack_l4proto_udp4，会update连接conn track的age，保证不ageout
        对于nf_conntrack_l4proto_tcp4，会有更复杂的操作。
     */
    ret = l4proto->packet(ct, skb, dataoff, ctinfo, pf, hooknum);
    if (ret <= 0) {
        /* Invalid: inverse of the return code tells
         * the netfilter core what to do */
        pr_debug("nf_conntrack_in: Can't track with proto module\n");
        nf_conntrack_put(skb->nfct);
        skb->nfct = NULL;
        NF_CT_STAT_INC_ATOMIC(net, invalid);
        if (ret == -NF_DROP)
            NF_CT_STAT_INC_ATOMIC(net, drop);
        ret = -ret;
        goto out;
    }
     
     /* 设置conn track的事件，不明白这里为什么叫做cache */
    if (set_reply && !test_and_set_bit(IPS_SEEN_REPLY_BIT, &ct->status))
        nf_conntrack_event_cache(IPCT_REPLY, ct);
out:
    if (tmpl)
        nf_ct_put(tmpl);

    return ret;
}

==============================================================================

/* On success, returns conntrack ptr, sets skb->nfct and ctinfo */
static inline struct nf_conn *
resolve_normal_ct(struct net *net, struct nf_conn *tmpl,
         struct sk_buff *skb,
         unsigned int dataoff,
         u_int16_t l3num,
         u_int8_t protonum,
         struct nf_conntrack_l3proto *l3proto,
         struct nf_conntrack_l4proto *l4proto,
         int *set_reply,
         enum ip_conntrack_info *ctinfo)
{
    struct nf_conntrack_tuple tuple;
    struct nf_conntrack_tuple_hash *h;
    struct nf_conn *ct;
    u16 zone = tmpl ? nf_ct_zone(tmpl) : NF_CT_DEFAULT_ZONE;

     /* 
        通过具体的L3和L4 target得到tuple
        TCP/UDP: 源IP 目的IP 源端口 目的端口 序列号
        ICMP: 源IP 目的IP ICMP类型 ICMP_Code 序列号
     */
    if (!nf_ct_get_tuple(skb, skb_network_offset(skb),
             dataoff, l3num, protonum, &tuple, l3proto,
             l4proto)) {
        pr_debug("resolve_normal_ct: Can't get tuple\n");
        return NULL;
    }

    /* look for tuple match */
    /* 通过tuple找到正确的conn track
       返回值为struct nf_conntrack_tuple_hash *
       对源的比较：
            (nf_inet_addr_cmp(&t1->src.u3, &t2->src.u3) &&
                    t1->src.u.all == t2->src.u.all &&
                    t1->src.l3num == t2->src.l3num)
       对目的地址的比较：
            (nf_inet_addr_cmp(&t1->dst.u3, &t2->dst.u3) &&
                    t1->dst.u.all == t2->dst.u.all &&
                    t1->dst.protonum == t2->dst.protonum);
    */
    h = nf_conntrack_find_get(net, zone, &tuple);
    if (!h) {
        /* 
            没有conn track的话，就创建一个新的conn track，并添加到hash表中
            init_conntrack中的动作包括：
                创建链接跟踪项nf_conn
                ct = __nf_conntrack_alloc(net, zone, tuple, &repl_tuple, GFP_ATOMIC, hash);
                为链接跟踪项建立4层相关的链接记录模块                
                l4proto->new(ct, skb, dataoff, timeouts)
                返回链接跟踪当前的节点
                return &ct->tuplehash[IP_CT_DIR_ORIGINAL];
                
        */
        h = init_conntrack(net, tmpl, &tuple, l3proto, l4proto,
                 skb, dataoff);
        if (!h)
            return NULL;
        if (IS_ERR(h))
            return (void *)h;
    }
    /* 
        从hash节点获得conn track 	
        返回值为struct nf_conn *
        static inline struct nf_conn *
        nf_ct_tuplehash_to_ctrack(const struct nf_conntrack_tuple_hash *hash) {
            return container_of(hash, struct nf_conn,
                    tuplehash[hash->tuple.dst.dir]);
        }

        计算的方法其实很简单，就是用该成员变量的指针减去它相对于结构体起始位置的偏移量。
        #define container_of(ptr, type, member) ({                      \
          const typeof( ((type *)0)->member ) *__mptr = (ptr);    \
          (type *)( (char *)__mptr - offsetof(type,member) );})
    */
    ct = nf_ct_tuplehash_to_ctrack(h);

    /* It exists; we have (non-exclusive) reference. */
    if (NF_CT_DIRECTION(h) == IP_CT_DIR_REPLY) {
        /* 
            tuple为REPLY方向，那么ctinfo为establish+reply 
        */
        *ctinfo = IP_CT_ESTABLISHED + IP_CT_IS_REPLY;

        /* Please set reply bit if this packet OK */
        *set_reply = 1;

    } else {
        /* 
            tuple为original方向，即初始的发送方向 
        */
        /* Once we've had two way comms, always ESTABLISHED. */
        if (test_bit(IPS_SEEN_REPLY_BIT, &ct->status)) {

            /* 之前收到过REPLY，那么ctinfo为established*/
            pr_debug("nf_conntrack_in: normal packet for %p\n", ct);
            *ctinfo = IP_CT_ESTABLISHED;

        } else if (test_bit(IPS_EXPECTED_BIT, &ct->status)) {

            pr_debug("nf_conntrack_in: related packet for %p\n", ct);

            /* 表明这是一个相关的conn track。如ICMP error或者FTP的data session？ */
            *ctinfo = IP_CT_RELATED;

        } else {

            /* 表明这是一个新的conn track*/
            pr_debug("nf_conntrack_in: new packet for %p\n", ct);
            *ctinfo = IP_CT_NEW;
        }
        *set_reply = 0;
    }
    skb->nfct = &ct->ct_general;
    skb->nfctinfo = *ctinfo;
    return ct;
}

==============================================================================

help模块可以用比较小的代价完成对链接跟踪功能的拓展，如需要对即将离开netfilter的数据包进行修改
help模块以比较低的优先级注册在LOCAL_OUT和POST_ROUTING两个hook点上

使用help模块需要实例化一个struct nf_conntrack_helper{}结构体实例，
然后使用nf_conntrack_helper_register()函数将其注册到helpers全局变量中，

helpers是一个全局的双向链表，里面保存了当前已经注册到链接跟踪系统中的所有协议的辅助模块；
全局helpers变量的定义和初始化在net/netfilter/nf_conntrack_helper.c文件中完成的
注册在Netfilter框架里LOCAL_OUT和POST_ROUTING两个hook点上nf_conntrack_help()回调函数所做的事情:
通过依次遍历helpers链表，然后调用每个ip_conntrack_helper{}对象的help()函数

很多协议的控制信息在应用层数据中被包含，这些信息直接影响到了链路的建立，比如ftp协议就是这样，
ftp分为port模式(主动)和passive模式(被动)，

port模式下，起初client连接server的21端口，然后当需要传输data的时候，client发送一个控制包给server，
包中包含client端开启的端口和自己的ip地址，server收到之后用自己的20端口去连接client控制包中建议的ip和端口，
在这种情况下，如果client在nat后面使用私网地址，那么server将无法连接client，
因此nat网关必须要处理这种情况，处理方式就是修改client发给server的控制包(如果加密将不可能修改，还好ftp是不加密的)；

在pass模式下，client连接server的21端口后，如果要传输data，client还要连接server的另一个随机端口，
该端口是由server发送的控制包传给client的，如果client或者server端所在的防火墙禁止了任意非熟知端口，那么数据将被防火墙拦截；

不管是port模式还是pass模式，防火墙都要处理“第二个”数据连接通路的放行问题，
在linux中是通过RELATED状态来放行的，正如前文所述，只需配置一条--state RELATED -j ACCEPT规则即可

每一个ip_conntrack都可以拥有多个helper，用于帮助处理连接相关的信息，
比如ftp协议穿越防火墙就需要处理nat和副连接(data连接)问题，
因此就有必要用一个helper模块来处理这一类情况，
处理ftp nat的helper和处理副连接的helper其实不是一类helper，
前者是ip_nat_ftp结构体，后者是ip_conntrack_ftp结构体，
虽然不同，但是它们的处理逻辑和注册逻辑都是一样的

ip_conntrack和ip_nat处理ftp总的过程就是，
ip_conntrack模块得到了连接的信息，然后根据连接信息可以得到一个helper，
需要说明，一个连接完全可以没有helper，而且大多数的都没有helper，是否需要helper是根据连接的类型决定的，
一般触及应用层控制数据的修改时才会使用helper，比如ftp的控制命令是在应用层数据中被传输的，
连接的类型是可以从数据包以及协议头中得到的，因此ip_conntrack模块需要数据包不能分段，也就是说需要完整的ip数据包。

得到helper之后开始调用其help函数，然后判断当前数据包是否需要help，
比如判断是否是ftp的特殊命令，该命令可以建立一条新的连接，
如果是这样的话，那么help函数则“预测”到一条即将建立的连接并将之和当前连接关联，然后ip_conntrack基本就没有什么做的了，

数据包继续在netfilter中流动，进入nat，同样的，nat也如ip_conntrack判断是否需要help，
如果是则调用helper的help函数，判断是否需要help的依据一般就是是否在ip_conntrack模块中“预测”到了即将建立的连接，
如果预测到了，那么就调用nat的helper的help函数，
并且将预测到的连接参数传入，在ip_nat_ftp的help函数中根据预测连接的信息对应用层控制数据进行修改。


helper模块一般是对需要在应用层数据中传输控制数据的协议进行帮助的，因为OS实现的协议栈并不包含应用层，
但是有的时候必须对应用层控制数据进行修改，这时就不得不需要一个额外的帮助模块了，
注意，一般helper修改的都是控制数据，而不是业务数据，所谓控制数据就是和业务无关的，仅仅影响到连接本身的数据。

==============================================================================
链接跟踪查找函数跟踪：
nf_conntrack_find_get
    __nf_conntrack_find_get
        ____nf_conntrack_find
            nf_ct_key_equal
                nf_ct_tuple_equal
                    __nf_ct_tuple_src_equal __nf_ct_tuple_dst_equal

所有的链接跟踪存储在&net->ct.hash桶中
hlist_nulls_for_each_entry_rcu(h, n, &net->ct.hash[bucket], hnnode) 

123 static inline bool __nf_ct_tuple_src_equal(const struct nf_conntrack_tuple *t1,
124                        const struct nf_conntrack_tuple *t2)
125 {
126     return (nf_inet_addr_cmp(&t1->src.u3, &t2->src.u3) &&
127         t1->src.u.all == t2->src.u.all &&
128         t1->src.l3num == t2->src.l3num);
129 }
130 
131 static inline bool __nf_ct_tuple_dst_equal(const struct nf_conntrack_tuple *t1,
132                        const struct nf_conntrack_tuple *t2)
133 {
134     return (nf_inet_addr_cmp(&t1->dst.u3, &t2->dst.u3) &&
135         t1->dst.u.all == t2->dst.u.all &&
136         t1->dst.protonum == t2->dst.protonum);
137 }


