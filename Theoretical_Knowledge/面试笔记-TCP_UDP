tcp和udp的区别，tcp是怎么做错误处理的?
1. 基于连接vs无连接:
UDP是无连接的协议，和点对点连接之前不需要发送消息。这就是为什么，UDP更加适合消息的多播发布，从单个点向多个点传输消息。

2. 可靠性不同

3.有序性
TCP也保证了消息的有序性。该消息将以从服务器端发出的同样的顺序发送到客户端，尽管这些消息到网络的另一端时可能是无序的。TCP协议将会为你排好
序。UDP不提供任何有序性或序列性的保证。数据包将以任何可能的顺序到达。

4.数据边界
TCP不保存数据的边界，而UDP保证。在传输控制协议，数据以字节流的形式发送，并没有明显的标志表明传输信号消息（段）的边界。

5.速度
TCP速度比较慢，而UDP速度比较快，因为TCP必须创建连接，以保证消息的可靠交付和有序性，他需要做比UDP多的多的事。

6.重量级vs轻量级
TCP被认为是重量级的协议，而与之相比，UDP协议则是一个轻量级的协议。因为UDP传输的信息中不承担任何间接创造连接，保证交货或秩序的的信息。这
也反映在用于承载元数据的头的大小。

7. 头大小
TCP具有比UDP更大的头。一个TCP数据包报头的大小是20字节，UDP数据报报头是8个字节。TCP报头中包含序列号，ACK号，数据偏移量，保留，控制位，
窗口，紧急指针，可选项，填充项，校验位，源端口和目的端口。而UDP报头只包含长度，源端口号，目的端口，和校验和

8. 拥塞或流控制
TCP有流量控制。在任何用户数据可以被发送之前，TCP需要三数据包来设置一个套接字连接。TCP处理的可靠性和拥塞控制。另一方面，UDP不能进行流量控
制。

==============================================================================

流量控制和拥塞控制的实现机制：
1）TCP采用大小可变的滑动窗口机制实现流量控制功能。窗口的大小是字节。在TCP报文段首部的窗口字段写入的数值就是当前给对方设置发送窗口的数据的
上限。
在数据传输过程中，TCP提供了一种基于滑动窗口协议的流量控制机制，用接收端接收能力（缓冲区的容量）的大小来控制发送端发送的数据量。
2）采用滑动窗口机制还可对网络进行拥塞控制，将网络中的分组（TCP报文段作为其数据部分）数量维持在一定的数量之下，当超过该数值时，网络的性能
会急剧恶化。
传输层的拥塞控制有慢启动（Slow-Start）、拥塞避免（Congestion Avoidance）、快重传（Fast Retransmit）和快恢复（Fast Recovery）
四种算法。
拥塞：　大量数据报涌入同一交换节点（如路由器），导致该节点资源耗尽而必须丢弃后面到达的数据报时，就是拥塞。

==============================================================================

重传机制：
TCP每发送一个报文段，就设置一次定时器。只要定时器设置的重发时间到而还没有收到确认，就要重发这一报文段。 
TCP环境
报文往返时间不定、有很大差别
A、B在一个局域网络，往返时延很小
A、C在一个互联网内，往返时延很大
因此，A很难确定一个固定的、与B、C通信都适用的定时器时间
TCP采用了一种自适应算法。这种算法记录每一个报文段发出的时间，以及收到相应的确认报文段的时间。这两个时间之差就是报文段的往返时延。将各个报
文段的往返时延样本加权平均，就得出报文段的平均往返时延T。

==============================================================================

滑动窗口机制：
TCP 采用大小可变的滑动窗口进行流量控制。窗口大小的单位是字节。
在 TCP 报文段首部的窗口字段写入的数值就是当前给对方设置的发送窗口数值的上限。发送窗口在连接建立时由双方商定。但在通信的过程中，接收端可根
据自己的资源情况，随时动态地调整对方的发送窗口上限值(可增大或减小)。

==============================================================================

TIME_WAIT状态产生场景\理由\如何避免:

1. 主动关闭的Socket端会进入TIME_WAIT状态，并且持续2MSL时间长度，MSL就是maximum segment lifetime(最大分节生命期）；
这是一个IP数据包能在互联网上生存的最长时间，超过这个时间将在网络中消失。
MSL在RFC 1122上建议是2分钟，而源自berkeley的TCP实现传统上使用30秒，因而，TIME_WAIT状态一般维持在1-4分钟。

2. 主动关闭的一方在发送最后一个ack 后就会进入TIME_WAIT 状态 停留2MSL（max segment lifetime）时间这个是TCP/IP必不可少的，也就
是“解决”不了的。

3. TIME_WAIT状态存在的理由：
    1）防止上一次连接中的包，迷路后重新出现，影响新连接
    （经过2MSL，上一次连接中所有的重复包都会消失）
    2）可靠的关闭TCP连接
        在进行关闭连接四路握手协议时，最后的ACK是由主动关闭端发出的，如果这个最终的ACK丢失，服务器将重发最终的FIN，因此客户端必须维护
状态信息允 许它重发最终的ACK。
    如果不维持这个状态信息，那么客户端将响应RST分节，服务器将此分节解释成一个错误（在java中会抛出connection reset的
SocketException)。
     因而，要实现TCP全双工连接的正常终止，必须处理终止序列四个分节中任何一个分节的丢失情况，主动关闭 的客户端必须维持状态信息进入
TIME_WAIT状态

4.time_wait状态如何避免
首先服务器可以设置SO_REUSEADDR套接字选项来通知内核，如果端口忙，当TCP连接位于TIME_WAIT状态时可以重用端口。在一个非常有用的场景就是，
如果你的服务器程序停止后想立即重启，而新的套接字依旧希望使用同一端口，此时SO_REUSEADDR选项就可以避免TIME_WAIT状态。

==============================================================================

tcp连接建立的时候3次握手的具体过程，以及每一步原因：

（1）第一步：源主机A的TCP向主机B发出连接请求报文段，其首部中的SYN(同步)标志位应置为1，表示想与目标主机B进行通信，并发送一个同步序列号X
(例：SEQ=100)进行同步，表明在后面传送数据时的第一个数据字节的序号是X＋1（即101）。SYN同步报文会指明客户端使用的端口以及TCP连接的初始序
号。

（2）第二步：目标主机B的TCP收到连接请求报文段后，如同意，则发回确认。在确认报中应将ACK位和SYN位置1，表示客户端的请求被接受。确认号应为X
＋1(图中为101)，同时也为自己选择一个序号Y。

（3）第三步：源主机A的TCP收到目标主机B的确认后要向目标主机B给出确认，其ACK置1，确认号为Y＋1，而自己的序号为X＋1。TCP的标准规定，SYN置
1的报文段要消耗掉一个序号。

运行客户进程的源主机A的TCP通知上层应用进程，连接已经建立。当源主机A向目标主机B发送第一个数据报文段时，其序号仍为X＋1，因为前一个确认报文
段并不消耗序号。
当运行服务进程的目标主机B的TCP收到源主机A的确认后，也通知其上层应用进程，连接已经建立。至此建立了一个全双工的连接。

==============================================================================

TCP初始化序列号不能设置为一个固定值，因为这样容易被攻击者猜出后续序列号，从而遭到攻击。

RFC1948中提出了一个较好的初始化序列号ISN随机生成算法。

ISN = M + F(localhost, localport, remotehost, remoteport).

M是一个计时器，这个计时器每隔4毫秒加1。

F是一个Hash算法，根据源IP、目的IP、源端口、目的端口生成一个随机数值。
要保证hash算法不能被外部轻易推算得出，用MD5算法是一个比较好的选择。

==============================================================================

tcp断开连接的具体过程，其中每一步是为什么那么做：

1)第一步：源主机A的应用进程先向其TCP发出连接释放请求，并且不再发送数据。TCP通知对方要释放从A到B这个方向的连接，将发往主机B的TCP报文段首
部的终止比特FIN置1，其序号X等于前面已传送过的数据的最后一个字节的序号加1。

2)第二步：目标主机B的TCP收到释放连接通知后即发出确认，其序号为Y，确认号为X＋1，同时通知高层应用进程，这样，从A到B的连接就释放了，连接处
于半关闭状态，相当于主机A向主机B说：“我已经没有数据要发送了。但如果还发送数据，我仍接收。”此后，主机B不再接收主机A发来的数据。但若主机B还
有一些数据要发送主机A，则可以继续发送。主机A只要正确收到数据，仍应向主机B发送确认。

3)第三步：若主机B不再向主机A发送数据，其应用进程就通知TCP释放连接。主机B发出的连接释放报文段必须将终止比特FIN和确认比特ACK置1，并使其序
号仍为Y，但还必须重复上次已发送过的ACK＝X＋1。

4) 第四步：主机A必须对此发出确认，将ACK置1，ACK＝Y＋1，而自己的序号是X＋1。这样才把从B到A的反方向的连接释放掉。主机A的TCP再向其应用进
程报告，整个连接已经全部释放。

==============================================================================

tcp建立连接和断开连接的各种过程中的状态转换细节：

客户端：主动打开SYN_SENT--->ESTABLISHED--->主动关闭FIN_WAIT_1--->FIN_WAIT_2--->TIME_WAIT

服务器端：LISTEN（被动打开）--->SYN_RCVD--->ESTABLISHED--->CLOSE_WAIT(被动关闭)--->LAST_ACK--->CLOSED

其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在
ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到
FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而
FIN_WAIT_2状态还有时常常可以用netstat看到。

==============================================================================

syn  flood是一种常见的DOS（denial of service拒绝服务）和Ddos(distributed denial of serivce 分布式拒绝服务）攻击方式。这是一
种使用TCP协议缺陷，发送大量的伪造的TCP连接请求，使得被攻击方cpu或内存资源耗尽，最终导致被攻击方无法提供正常的服务。

　　要明白这种攻击原理，还要从TCP连接的建立说起：

　　大家都知道，TCP和UDP不同，它提供一种基于连接的，可靠的字节流服务。想要通信的双方，都要首先建立一条TCP连接。这条连接的两端只有通信的双
方。TCP连接的建立是这样的：
　　首先，请求端（发送端）会发一个带有SYN标志位的报文，该报文中含有发送端的初始序号ISN（initinal sequence number)和发送端使用的端口
号，该报文就是请求建立连接，
　　其次，服务器收到这个请求报文后，就会回一个SYN+ACK的报文，同时这个报文中也包含服务器的ISN以及对请求端的确认序号，这个确认序号的值是
请求端的序号值+1，表示请求端的请求被接受，

　　最后，请求端收到这个报文后，就会回应给服务器一个ACK报文，到此一个TCP连接就建立了。

　　上面也就是典型的TCP三次握手过程（Three-way  Handshake)。问题就是在这最后一次的确认里，如果请求端由于某种异常（死机或掉线），服务
器没有收到请求端发送的回应ACK。那么第三次握手没有完成，服务器就会向请求端再次发送一个SYN+ACK报文，并等待一段时间后丢弃这个未完成的连接。
这个时间长度称为SYN Timeout，一般来说是分钟的数量级（大约30秒到2分钟）；
一个用户出现异常导致服务器等待一分钟是没有什么问题的。如果有恶意攻击者采用这种方式，控制大量的肉鸡来模拟这种情况，服务器端就要去维护一个大
量的半连接表而消耗大量的cpu和内存资源。服务器会对这个半连接表进行一个遍历，然后尝试发送SYN+ACK来继续TCP连接的建立。实际上如果客户的TCP
协议栈如果不够强大，最后的结果是服务器堆栈溢出崩溃。即使服务器端足够的强大，服务器也会因为忙于处理攻击者的TCP连接请求而无瑕理会正常的客户
的请求，此时从客户端来看，服务器就已经失去响应，这时我们称做服务器遭受了SYN Flood攻击。

==============================================================================
send函数缓存问题
send()函数默认情况下会使用Nagle算法。Nagle算法通过将未确认的数据存入缓冲区直到积攒到一定数量一起发送的方法。来降低主机发送零碎小数据包
的数目。所以假设send()函数发送数据过快的话，该算法会将一些数据打包后统一发出去。假设不了解这样的情况，接收端採会遇到看似非常奇怪的问题，
比方成功recv()的次数与成功send()的次数不相等。在这中情况下，接收端能够通过recv()的返回值是否为0来推断发送端是否发送完成。

==============================================================================
Nagle算法：
是为了减少广域网的小分组数目，从而减小网络拥塞的出现；
该算法要求一个tcp连接上最多只能有一个未被确认的未完成的小分组，在该分组ack到达之前不能发送其他的小分组，tcp需要收集这些少量的分组，并在
ack到来时以一个分组的方式发送出去；其中小分组的定义是小于MSS的任何分组；
该算法的优越之处在于它是自适应的，确认到达的越快，数据也就发哦送的越快；而在希望减少微小分组数目的低速广域网上，则会发送更少的分组；

设计规则如下：

　　（1）如果包长度达到最大报文长度（MSS，Maximum Segment Size），则允许发送；

　　（2）如果该包含有FIN，则允许发送；

　　（3）设置了TCP_NODELAY选项，则允许发送；

　　（4）未设置TCP_CORK选项时，若所有发出去的小数据包（包长度小于MSS）均被确认，则允许发送；

　　（5）上述条件都未满足，但发生了超时（一般为200ms），则立即发送。


关闭Nagle：
setsockopt(fd,IPPROTO_TCP,TCP_NODELAY,(char*)&flag,sizeof(flag));

==============================================================================
延迟ACK：
如果tcp对每个数据包都发送一个ack确认，那么只是一个单独的数据包为了发送一个ack代价比较高，所以tcp会延迟一段时间，如果这段时间内有数据发
送到对端，则捎带发送ack，如果在延迟ack定时器触发时候，发现ack尚未发送，则立即单独发送；
延迟ACK好处：
(1) 避免糊涂窗口综合症；
(2) 发送数据的时候将ack捎带发送，不必单独发送ack；
(3) 如果延迟时间内有多个数据段到达，那么允许协议栈发送一个ack确认多个报文段；

==============================================================================
当Nagle遇上延迟ACK：
试想如下典型操作，写-写-读，即通过多个写小片数据向对端发送单个逻辑的操作，两次写数据长度小于MSS，当第一次写数据到达对端后，对端延迟ack，
不发送ack，而本端因为要发送的数据长度小于MSS，所以nagle算法起作用，数据并不会立即发送，而是等待对端发送的第一次数据确认ack；这样的情况
下，需要等待对端超时发送ack，然后本段才能发送第二次写的数据，从而造成延迟；

由于有Nagle算法，如果发送端启用了Nagle算法，接收端启用了TCP Delayed Acknowledge。当发送端发起两次写一次读的时候，第一次写，由于TCP
没有等待ACK，直接发出去了，而第二次写的时候，第一次写的ACK还没有接收到，从而等待；而接收端有Delayed Acknowledge机制，会等待40ms以提
供合并多个ACK的机会。Nagle算法的使用在一些实时性要求比较高的场合，会引起一些问题。比如项目中设计的UI鼠标远程控制远端的机器时，发现远端的
鼠标操作很卡顿，这是因为鼠标消息的发送端由于Nagle算法的默认开启，是有延迟的，

==============================================================================
如下场景考虑关闭Nagle算法：
(1) 对端不向本端发送数据，并且对延时比较敏感的操作；这种操作没法捎带ack；
(2) 如上写-写-读操作；对于此种情况，优先使用其他方式，而不是关闭Nagle算法：
--使用writev，而不是两次调用write，单个writev调用会使tcp输出一次而不是两次，只产生一个tcp分节，这是首选方法；
--把两次写操作的数据复制到单个缓冲区，然后对缓冲区调用一次write；
--关闭Nagle算法，调用write两次；有损于网络，通常不考虑；
setsockopt(fd,IPPROTO_TCP,TCP_NODELAY,(char*)&flag,sizeof(flag));

==============================================================================

epoll与select的区别：

问题的引出，当需要读两个以上的I/O的时候，如果使用阻塞式的I/O，那么可能长时间的阻塞在一个描述符上面，另外的描述符虽然有数据但是不能读出
来，这样实时性不能满足要求，大概的解决方案有以下几种：

1.使用多进程或者多线程，但是这种方法会造成程序的复杂，而且对与进程与线程的创建维护也需要很多的开销。（Apache服务器是用的子进程的方式，优
点可以隔离用户）

2.用一个进程，但是使用非阻塞的I/O读取数据，当一个I/O不可读的时候立刻返回，检查下一个是否可读，这种形式的循环为轮询（polling），这种方法
比较浪费CPU时间，因为大多数时间是不可读，但是仍花费时间不断反复执行read系统调用。

3.异步I/O（asynchronous I/O），当一个描述符准备好的时候用一个信号告诉进程，但是由于信号个数有限，多个描述符时不适用。

4.一种较好的方式为I/O多路转接（I/O multiplexing）（貌似也翻译多路复用），先构造一张有关描述符的列表（epoll中为队列），然后调用一个函
数，直到这些描述符中的一个准备好时才返回，返回时告诉进程哪些I/O就绪。select和epoll这两个机制都是多路I/O机制的解决方案，select为POSIX
标准中的，而epoll为Linux所特有的。

区别（epoll相对select优点）主要有三：

1.select的句柄数目受限，在linux/posix_types.h头文件有这样的声明：#define __FD_SETSIZE    1024  表示select最多同时监听1024
个fd。而epoll没有，它的限制是最大的打开文件句柄数目。

2.epoll的最大好处是不会随着FD的数目增长而降低效率，在selec中采用轮询处理，其中的数据结构类似一个数组的数据结构，而epoll是维护一个队
列，直接看队列是不是空就可以了。epoll只会对"活跃"的socket进行操作---这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。
那么，只有"活跃"的socket才会主动的去调用 callback函数（把这个句柄加入队列），其他idle状态句柄则不会，在这点上，epoll实现了一
个"伪"AIO。但是如果绝大部分的I/O都是“活跃的”，每个I/O端口使用率很高的话，epoll效率不一定比select高（可能是要维护队列复杂）。

3.使用mmap加速内核与用户空间的消息传递。无论是select,poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存拷贝就很重
要，在这点上，epoll是通过内核于用户空间mmap同一块内存实现的。

==============================================================================
epoll中et和lt的区别与实现原理：

epoll有2种工作方式:LT和ET。
LT(level triggered 水平触发)是缺省的工作方式，并且同时支持block和no-block socket。
在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。
如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。
传统的select/poll都是这种模型的代表。

ET (edge-triggered 边缘触发)是高速工作方式，只支持no-block socket。
在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。
然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述 符发送更多的就绪通知，直到你做了某些操作
导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致 了一个
EWOULDBLOCK 错误）。
但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only 
once),不过在TCP协议中，ET模式的加速效用仍需要更多的benchmark确认。

epoll只有epoll_create,epoll_ctl,epoll_wait 3个系统调用。

==============================================================================

单机最大tcp连接数

在tcp应用中，server事先在某个固定端口监听，client主动发起连接，经过三路握手后建立tcp连接。那么对单机，其最大并发tcp连接数是多少？
如何标识一个TCP连接

在确定最大连接数之前，先来看看系统如何标识一个tcp连接。系统用一个4四元组来唯一标识一个TCP连接：{local ip, local port,remote 
ip,remote port}。

client最大tcp连接数

client每次发起tcp连接请求时，除非绑定端口，通常会让系统选取一个空闲的本地端口（local port），该端口是独占的，不能和其他tcp连接共享。
tcp端口的数据类型是unsigned short，因此本地端口个数最大只有65536，端口0有特殊含义，不能使用，这样可用端口最多只有65535，所以在全部作
为client端的情况下，最大tcp连接数为65535，这些连接可以连到不同的server ip。

server最大tcp连接数

server通常固定在某个本地端口上监听，等待client的连接请求。不考虑地址重用（unix的SO_REUSEADDR选项）的情况下，即使server端有多个ip，
本地监听端口也是独占的，因此server端tcp连接4元组中只有remote ip（也就是client ip）和remote port（客户端port）是可变的，因此最大
tcp连接为客户端ip数×客户端port数，对IPV4，不考虑ip地址分类等因素，最大tcp连接数约为2的32次方（ip数）×2的16次方（port数），也就是
server端单机最大tcp连接数约为2的48次方。

实际的tcp连接数

上面给出的是理论上的单机最大连接数，在实际环境中，受到机器资源、操作系统等的限制，特别是sever端，其最大并发tcp连接数远不能达到理论上限。
在unix/linux下限制连接数的主要因素是内存和允许的文件描述符个数（每个tcp连接都要占用一定内存，每个socket就是一个文件描述符），另外1024
以下的端口通常为保留端口。在默认2.6内核配置下，经过试验，每个socket占用内存在15~20k之间。

影响一个socket占用内存的参数包括：

rmem_max

wmem_max

tcp_rmem

tcp_wmem

tcp_mem

grep skbuff /proc/slabinfo

对server端，通过增加内存、修改最大文件描述符个数等参数，单机最大并发TCP连接数超过10万 是没问题的

==============================================================================

https://blog.csdn.net/russell_tao/article/details/18711023

linux上还提供了以下系统级的配置来整体设置服务器上的TCP内存使用

sysctl -a
net.ipv4.tcp_rmem = 8192 87380 16777216  
net.ipv4.tcp_wmem = 8192 65536 16777216  
net.ipv4.tcp_mem = 8388608 12582912 16777216  
net.core.rmem_default = 262144  
net.core.wmem_default = 262144  
net.core.rmem_max = 16777216  
net.core.wmem_max = 16777216

net.ipv4.tcp_moderate_rcvbuf = 1  
net.ipv4.tcp_adv_win_scale = 2

应用程序编程时可以设置setsockopt()、SO_SNDBUF、SO_RCVBUF

==============================================================================

先从应用程序编程时可以设置的SO_SNDBUF、SO_RCVBUF说起

无论何种语言，都对TCP连接提供基于setsockopt方法实现的SO_SNDBUF、SO_RCVBUF，怎么理解这两个属性的意义呢？
SO_SNDBUF、SO_RCVBUF都是个体化的设置，即，只会影响到设置过的连接，而不会对其他连接生效。SO_SNDBUF表示这个连接上的内核写缓存上限。实
际上，进程设置的SO_SNDBUF也并不是真的上限，在内核中会把这个值翻一倍再作为写缓存上限使用，我们不需要纠结这种细节，只需要知道，当设置了
SO_SNDBUF时，就相当于划定了所操作的TCP连接上的写缓存能够使用的最大内存。然而，这个值也不是可以由着进程随意设置的，它会受制于系统级的上下
限，当它大于上面的系统配置wmem_max（net.core.wmem_max）时，将会被wmem_max替代（同样翻一倍）；而当它特别小时，例如在2.6.18内核中设
计的写缓存最小值为2K字节，此时也会被直接替代为2K。

SO_RCVBUF表示连接上的读缓存上限，与SO_SNDBUF类似，它也受制于rmem_max配置项，实际在内核中也是2倍大小作为读缓存的使用上限。SO_RCVBUF
设置时也有下限，同样在2.6.18内核中若这个值小于256字节就会被256所替代。

==============================================================================

可以设置的SO_SNDBUF、SO_RCVBUF缓存使用上限与实际内存到底有怎样的关系呢？

TCP连接所用内存主要由读写缓存决定，而读写缓存的大小只与实际使用场景有关，在实际使用未达到上限时，SO_SNDBUF、SO_RCVBUF是不起任何作用
的。对读缓存来说，接收到一个来自连接对端的TCP报文时，会导致读缓存增加，当然，如果加上报文大小后读缓存已经超过了读缓存上限，那么这个报文会
被丢弃从而读缓存大小维持不变。什么时候读缓存使用的内存会减少呢？当进程调用read、recv这样的方法读取TCP流时，读缓存就会减少。因此，读缓存
是一个动态变化的、实际用到多少才分配多少的缓冲内存，当这个连接非常空闲时，且用户进程已经把连接上接收到的数据都消费了，那么读缓存使用内存就
是0。

写缓存也是同样道理。当用户进程调用send或者write这样的方法发送TCP流时，就会造成写缓存增大。当然，如果写缓存已经到达上限，那么写缓存维持
不变，向用户进程返回失败。而每当接收到TCP连接对端发来的ACK确认了报文的成功发送时，写缓存就会减少，这是因为TCP的可靠性决定的，发出去报文
后由于担心报文丢失而不会销毁它，可能会由重发定时器来重发报文。因此，写缓存也是动态变化的，空闲的正常连接上，写缓存所用内存通常也为0。

因此，只有当接收网络报文的速度大于应用程序读取报文的速度时，可能使读缓存达到了上限，这时这个缓存使用上限才会起作用。所起作用为：丢弃掉新收
到的报文，防止这个TCP连接消耗太多的服务器资源。同样，当应用程序发送报文的速度大于接收对方确认ACK报文的速度时，写缓存可能达到上限，从而使
send这样的方法失败，内核不为其分配内存。
==============================================================================

缓存的大小与TCP的滑动窗口到底有什么关系？

读缓存的作用有2个：1、将无序的、落在接收滑动窗口内的TCP报文缓存起来；2、当有序的、可以供应用程序读取的报文出现时，由于应用程序的读取是延
时的，所以会把待应用程序读取的报文也保存在读缓存中。所以，读缓存一分为二，一部分缓存无序报文，一部分缓存待延时读取的有序报文。这两部分缓存
大小之和由于受制于同一个上限值，所以它们是会互相影响的，当应用程序读取速率过慢时，这块过大的应用缓存将会影响到套接字缓存，使接收滑动窗口缩
小，从而通知连接的对端降低发送速度，避免无谓的网络传输。当应用程序长时间不读取数据，造成应用缓存将套接字缓存挤压到没空间，那么连接对端会收
到接收窗口为0的通知，告诉对方：我现在消化不了更多的报文了。

TCP协议需要考虑复杂的网络环境，所以使用了慢启动、拥塞窗口（参见高性能网络编程2----TCP消息的发送），
建立连接时的初始窗口并不会按照接收缓存的最大值来初始化。这是因为，过大的初始窗口从宏观角度，对整个网络可能造成过载引发恶性循环，也就是考虑
到链路上各环节的诸多路由器、交换机可能扛不住压力不断的丢包（特别是广域网），而微观的TCP连接的双方却只按照自己的读缓存上限作为接收窗口，这
样双方的发送窗口（对方的接收窗口）越大就对网络产生越坏的影响。慢启动就是使初始窗口尽量的小，随着接收到对方的有效报文，确认了网络的有效传输
能力后，才开始增大接收窗口。

不同的linux内核有着不同的初始窗口。

将1500字节的MTU去除了20字节的IP头、20字节的TCP头以后，一个最大报文能够承载的有效数据长度1460。
但有些网络中，会在TCP的可选头部里，使用12字节作为时间戳使用，这样，有效数据就是MSS再减去12，初始窗口就是（1460-12）*4=5792，这与窗口
想表达的含义是一致的，即：我能够处理的有效数据长度。

当窗口从初始窗口一路扩张到最大接收窗口时，最大接收窗口就是最大读缓存吗？
不是，因为必须分一部分缓存用于应用程序的延时报文读取。到底会分多少出来呢？这是可配的系统选项，如下：
net.ipv4.tcp_adv_win_scale = 2 

这里的tcp_adv_win_scale意味着，将要拿出1/(2^tcp_adv_win_scale)缓存出来做应用缓存。即，默认tcp_adv_win_scale配置为2时，就是拿
出至少1/4的内存用于应用读缓存，那么，最大的接收滑动窗口的大小只能到达读缓存的3/4。

最大读缓存到底应该设置到多少为合适呢？

当应用缓存所占的份额通过tcp_adv_win_scale配置确定后，读缓存的上限应当由最大的TCP接收窗口决定。初始窗口可能只有4个或者10个MSS，但在无
丢包情形下随着报文的交互窗口就会增大，当窗口过大时，“过大”是什么意思呢？即，对于通讯的两台机器的内存而言不算大，但是对于整个网络负载来说
过大了，就会对网络设备引发恶性循环，不断的因为繁忙的网络设备造成丢包。而窗口过小时，就无法充分的利用网络资源。所以，一般会以BDP来设置最大
接收窗口（可计算出最大读缓存）。BDP叫做带宽时延积，也就是带宽与网络时延的乘积，例如若我们的带宽为2Gbps，时延为10ms，那么带宽时延积BDP
则为2G/8*0.01=2.5MB，所以这样的网络中可以设最大接收窗口为2.5MB，这样最大读缓存可以设为4/3*2.5MB=3.3MB。

为什么呢？因为BDP就表示了网络承载能力，最大接收窗口就表示了网络承载能力内可以不经确认发出的报文。


经常提及的所谓长肥网络，“长”就是是时延长，“肥”就是带宽大，这两者任何一个大时，BDP就大，都应导致最大窗口增大，进而导致读缓存上限增大。所以
在长肥网络中的服务器，缓存上限都是比较大的。（当然，TCP原始的16位长度的数字表示窗口虽然有上限，但在RFC1323中定义的弹性滑动窗口使得滑动窗
口可以扩展到足够大。）

发送窗口实际上就是TCP连接对方的接收窗口，所以大家可以按接收窗口来推断，这里不再啰嗦。

==============================================================================

linux的TCP缓存上限自动调整策略

对于一个TCP连接来说，可能已经充分利用网络资源，使用大窗口、大缓存来保持高速传输了。比如在长肥网络中，缓存上限可能会被设置为几十兆字节，但
系统的总内存却是有限的，当每一个连接都全速飞奔使用到最大窗口时，1万个连接就会占用内存到几百G了，这就限制了高并发场景的使用，公平性也得不到
保证。我们希望的场景是，在并发连接比较少时，把缓存限制放大一些，让每一个TCP连接开足马力工作；当并发连接很多时，此时系统内存资源不足，那么
就把缓存限制缩小一些，使每一个TCP连接的缓存尽量的小一些，以容纳更多的连接。

linux为了实现这种场景，引入了自动调整内存分配的功能，由tcp_moderate_rcvbuf配置决定，如下：
net.ipv4.tcp_moderate_rcvbuf = 1
默认tcp_moderate_rcvbuf配置为1，表示打开了TCP内存自动调整功能。若配置为0，这个功能将不会生效（慎用）。

另外请注意：当我们在编程中对连接设置了SO_SNDBUF、SO_RCVBUF，将会使linux内核不再对这样的连接执行自动调整功能！
tcp_rmem[3]数组表示任何一个TCP连接上的读缓存上限，其中tcp_rmem[0]表示最小上限，tcp_rmem[1]表示初始上限（注意，它会覆盖适用于所有协
议的rmem_default配置），tcp_rmem[2]表示最大上限。
tcp_wmem[3]数组表示写缓存，与tcp_rmem[3]类似，不再赘述。

tcp_mem[3]数组就用来设定TCP内存的整体使用状况，所以它的值很大（它的单位也不是字节，而是页--4K或者8K等这样的单位！）。这3个值定义了
TCP整体内存的无压力值、压力模式开启阀值、最大使用值。以这3个值为标记点则内存共有4种情况：
(1)、当TCP整体内存小于tcp_mem[0]时，表示系统内存总体无压力。若之前内存曾经超过了tcp_mem[1]使系统进入内存压力模式，那么此时也会把压力
模式关闭。
这种情况下，只要TCP连接使用的缓存没有达到上限（注意，虽然初始上限是tcp_rmem[1]，但这个值是可变的，下文会详述），那么新内存的分配一定是
成功的。

(2)、当TCP内存在tcp_mem[0]与tcp_mem[1]之间时，系统可能处于内存压力模式，例如总内存刚从tcp_mem[1]之上下来；也可能是在非压力模式下，
例如总内存刚从tcp_mem[0]以下上来。
此时，无论是否在压力模式下，只要TCP连接所用缓存未超过tcp_rmem[0]或者tcp_wmem[0]，那么都一定都能成功分配新内存。否则，基本上就会面临分
配失败的状况。（注意：还有一些例外场景允许分配内存成功，由于对于我们理解这几个配置项意义不大，故略过。）

(3)、当TCP内存在tcp_mem[1]与tcp_mem[2]之间时，系统一定处于系统压力模式下。其他行为与上同。

(4)、当TCP内存在tcp_mem[2]之上时，毫无疑问，系统一定在压力模式下，而且此时所有的新TCP缓存分配都会失败。

当系统在非压力模式下，上面我所说的每个连接的读写缓存上限，才有可能增加，当然最大也不会超过tcp_rmem[2]或者tcp_wmem[2]。相反，在压力模式
下，读写缓存上限则有可能减少，虽然上限可能会小于tcp_rmem[0]或者tcp_wmem[0]。

所以，粗略的总结下，对这3个数组可以这么看：
1、只要系统TCP的总体内存超了 tcp_mem[2] ，新内存分配都会失败。
2、tcp_rmem[0]或者tcp_wmem[0]优先级也很高，只要条件1不超限，那么只要连接内存小于这两个值，就保证新内存分配一定成功。
3、只要总体内存不超过tcp_mem[0]，那么新内存在不超过连接缓存的上限时也能保证分配成功。
4、tcp_mem[1]与tcp_mem[0]构成了开启、关闭内存压力模式的开关。在压力模式下，连接缓存上限可能会减少。在非压力模式下，连接缓存上限可能会
增加，最多增加到tcp_rmem[2]或者tcp_wmem[2]。

==============================================================================

net.ipv4.tcp_wmem = 4096 16384 4194304
net.ipv4.tcp_rmem = 4096 87380 4194304

就是说，每个tcp连接的socket，至少需要8k字节，那么对于8G内存的机器，如果不考虑swap等其他因素，最多支持并发100万个tcp socket

每一个tcp连接占用的内存计算可以简化为：tcp写（发送）缓存+tcp读（接收）缓存。

因此，一个tcp连接在内核中需要占用的最小内存是：net.ipv4.tcp_wmem的最小值 + net.ipv4.tcp_rmem的最小值，根据默认配置，这个值就是 
4096+4096，也就是8k。因此，我们可以说默认情况一个tcp连接占用的内存应该是8k！

http://blog.csdn.net/fox_hacker/article/details/41440561

而对于内存，tcp连接归根结底需要双方接收和发送数据，那么就需要一个读缓冲区和写缓冲区，这两个buffer在linux下最小为4096字节，可通过
cat /proc/sys/net/ipv4/tcp_rmem和cat /proc/sys/net/ipv4/tcp_wmem来查看。所以，一个tcp连接最小占用内存为4096+4096 = 8k，

http://benpaozhe.blog.51cto.com/10239098/1752675

TCP读缓存大小，单位是字节：第一个是最小值4K，第二个是默认值85K，第三个是最大值16M，这个可以在sysctl.conf中net.ipv4.tcp_rmem中进行
调整。

TCP写缓存大小，单位是字节：第一个是最小值4K，第二个是默认值64K，第三个是最大值16M， 这个可以在sysctl.conf中net.ipv4.tcp_wmem中进
行调整。

也就是说一个TCP在三次握手建立连接后，最小的内存消耗在8K左右

==============================================================================

对于接收缓冲区和发送缓冲区，如果没有数据，是不占内存的。具体来说，对于接收缓冲区，只有当有数据可读但应用程序尚未读取的时候才占内存（就是 
epoll_wait 返回 EPOLL_IN之后，程序调用 read() 之前的那一小段时间）。换句话说，只要服务器总是及时读取数据，接收缓冲区基本不占内存。对
于发送缓冲区，只有等待发送的数据和发送之后尚未收到 ACK 的数据才占用内存，在稳态下，发送缓冲区占用的内存等于 BDP。比如考虑发送方每秒钟发 
1MB 数据，rtt 是 50ms 的情况，那么发送缓冲区平均占用 51.2 KB （不计 skbuff 的额外开销）。对于千兆网环境，一台单网口的机器最多支持 
112 个这样的连接，即便考虑 skbuff 的额外开销，所有这些连接的发送缓冲区一共占用不到 10MB 内存。（即便 rtt 高达 1s，那么千兆网的 
BDP 是 125MB，比起购买带宽的成本，这点内存开销可以忽略不计）。

服务端在收到 SYN 之后不会立刻创建 tcp_sock，而是会创建 tcp_request_sock 来处理三路握手，后者要小得多（256 字节），等到收到三路握手
最后的 ACK 才创建 tcp_sock。主动关闭 TCP socket 会进入 TIME_WAIT 状态，tcp_sock 会被释放，取而代之的是小得多的 
inet_timewait_sock 对象（192 字节），因此 TIME_WAIT 并不占用多少资源。总之，Linux 协议栈尽可能缩小 tcp_sock 的生命期，以节约内
存。

==============================================================================
Linux如何查看端口
lsof -i:端口号 用于查看某一端口的占用情况，比如查看8000端口使用情况，lsof -i:8000
netstat -tunlp |grep 端口号，用于查看指定的端口号的进程情况，如查看8000端口的情况，netstat -tunlp |grep 8000

==============================================================================

https://www.cnblogs.com/wanpengcoder/p/5356776.html
https://www.cnblogs.com/549294286/p/5208357.html
1. connect出错：
(1) 若TCP客户端没有收到syn分节的响应，则返回ETIMEOUT错误；调用connect函数时，内核发送一个syn，若无响应则等待6s后再发送一个，若仍然
无响应则等待24s后在发送一个，若总共等待75s后仍未收到响应则返回本错误；
(2) 若对客户的syn响应是rst，则表明该服务器在我们指定的端口上没有进程在等待与之连接，这是一种硬错误，客户一收到rst马上返回
ECONNREFUSED错误；
(3) 若客户发送的syn在中间的某个路由器上引发了目的不可达icmp错误，则认为是一种软错误。客户主机内核保存该消息，并按照第一种情况的时间间隔
继续发送syn，在某个规定时间后仍未收到响应，则把保存的消息作为EHOSTUNREACH或者ENETUNREACH错误返回给进程；

2. accept返回前连接中止：
在比较忙的服务器中，在建立三次握手之后，调用accept之前，可能出现客户端断开连接的情况；如，三次握手之后，客户端发送rst，然后服务器调用
accept。posix指出这种情况errno设置为CONNABORTED;
注意Berkeley实现中，没有返回这个错误，而是EPROTO，同时完成三次握手的连接会从已完成队列中移除；在这种情况下，如果我们用select监听到有新
的连接完成，但之后又被从完成队列中删除，此时如果调用阻塞accept就会产生阻塞；
解决办法：
(1) 使用select监听套接字是否有完成连接的时候，总是把这个监听套接字设置为非阻塞；
(2) 在后续的accept调用中忽略以下错误，EWOULDBLOCK(Berkeley实现，客户中止连接), ECONNABORTED(posix实现，客户中止连接), EPROTO
(serv4实现，客户中止连接)和EINTR(如果有信号被捕获)；

3. 服务器进程被终止(kill)：
在客户端和服务器端建立连接之后，使用kill命令杀死服务器进程，进程终止会关闭所有打开的描述符，这导致了其向客户端发送了一个FIN，而客户端则响
应了一个ack，这就完成了tcp连接终止的前半部分，只代表服务器不在发送数据了；但是客户端并不知道服务器端已经终止了，当客户端向服务器写数据的
时候，由于服务器进程终止，所以响应了rst，如果我们使用select等方式，能够立即知道当前连接状态；如下：
(1) 如果对端tcp发送数据，那么套接字可读，并且read返回一个大于0的值(读入字节数)；
(2) 如果对端tcp发送了fin(对端进程终止)，那么该套接字变为可读，并且read返回0(EOF)；
(3) 如果对端tcp发送rst(对端主机崩溃并重启)，那么该套接字变为可读，并且read返回-1，errno中含有确切错误码；

4.服务器主机崩溃：
建立连接之后，服务器主机崩溃，此时如果客户端发送数据，会发现客户端会在一定时间内持续重传，视图从服务器端收到数据的ack，当重传时间超过指定
时间后，服务器仍然没有响应，那么返回的是ETIMEDOUT；

5.服务器主机不可达： 
建立连接之后，服务器主机未崩溃，但是由于中间路由器故障灯，判定主机或网络不可达，此时如果客户端发送数据，会发现客户端会在一定时间内持续重
传，视图从服务器端收到数据的ack，当重传时间超过指定时间后，服务器仍然没有响应，那么返回的是EHOSTUNREACH或ENETUNREACH；

6.服务器主机崩溃后重启：
当服务器主机崩溃重启后，之前所有的tcp连接丢失，此时服务器若收到来自客户端的数据，会响应一个rst；
客户端调用read将返回一个ECONNRESET错误；

7.服务器主机关机：
系统关机时，init进程给所有进程发送SIGTERM信号，等待固定的时间，然后给所有仍在运行的进程发送SIGKILL信号，我们的进程会被SIGTERM或者
SIGKILL信号终止，所以与前面服务器进程终止相同，进程关闭所有描述符，并发送fin，完成服务器端的半关闭；

8.sigpipe信号：
当一个进程向某个收到rst的套接字执行写操作的时候，内核向该进程发送一个SIGPIPE信号，
该信号的默认行为是终止进程，因此进程必须捕获它以免不情愿的被终止；
不论进程是捕捉了该信号并从信号处理函数中返回，还是简单忽略该信号，写操作都将返回EPIPE错误；

==============================================================================
也就是socket连接的一端如何知道连接已经被动关闭，如另一端强退，另一端close等？
网络连接socket建立后，若某一端关闭连接，而另一端仍然向它写数据，第一次写数据后会收到RST响应，
此后再写数据，kernel将向进程发出SIGPIPE信号，通知进程此连接已经断开。
而SIGPIPE信号的默认处理是终止程序，导致上述问题的发生！
SIGPIPE：Broken pipe：向一个没有读端的管道写数据。默认动作为终止进程。
但是我并不希望终止进程，而是等待write返回错误，来判断connect断开，因此我需要忽略该信号。

要在服务器端判断客户端已经关闭了连接，可以用write的返回值判断。
但是要注意的是：write只是把服务端应用层中的数据发送给TCP层发送缓存，
然后由TCP层将应用层数据再发送给客户端TCP层，所以，当客户端关闭连接时，服务器端write并不会立刻返回-1。
通过抓包工具抓包发现，客户端向服务器端发送FIN之后，服务器端TCP层还是向客户端发出了几个报文的，不过会收到客户端的RST。
当服务端TCP层收到客户端TCP层发过来的RST报文之后，就会检测到对方已经关闭了连接，这时候再去write就会出错，中间是有一段延迟的！
==============================================================================

套接字的默认状态是阻塞的，这就意味着当发出一个不能立即完成的套接字调用时，其进程将被投入睡眠，等待响应操作完成，可能阻塞的套接字调用可分为
以下四类：

(1) 输入操作，包括read，readv，recv，recvfrom，recvmsg；
(2) 输出操作，包括write，writev，send，sendto，sendmsg；
(3) 接受外来连接，即accept函数。
(4) 发起外出连接，即tcp的connect函数；

==============================================================================
非阻塞connect:
当一个非阻塞的tcp套接字上调用connect时，connect将立即返回一个EINPROGRESS错误，不过已经发起的tcp三路握手继续进行。我们接着使用
select检测这个连接或成功或失败的已建立条件。非阻塞connect有三个用途：
(1) 我们可以把三路握手叠加在其他处理上，完成一个connect要花的RTT时间，而RTT波动很大，从局域网上的几毫秒到几百毫秒甚至是广域网的几秒。这
段时间内也许有我们想要执行的其他工作可执行；
(2) 我们可以使用这个技术同时建立多个连接；这个技术随着web浏览器流行起来；
(3) 既然使用select等待连接建立，我们可以给select指定一个时间限制，使得我们能够缩短connect的超时。

非阻塞connect细节：
(1) 尽管套接字是非阻塞的，如果连接到的服务器在同一个主机上，那么当我们调用connect时候，连接通常立刻建立，我们必须处理这种情形；
(2) 源自Berkeley的实现(和posix)有关select和非阻塞connect的以下两个原则:
--(a) 当连接成功建立时，描述符变为可写；
--(b) 当连接建立遇到错误时，描述符变为既可读又可写；

==============================================================================
非阻塞accept：

在比较忙的服务器中，在建立三次握手之后，调用accept之前，可能出现客户端断开连接的情况，再这样的情况下；如，三次握手之后，客户端发送rst，然
后服务器调用accept。posix指出这种情况errno设置为CONNABORTED;

注意Berkeley实现中，没有返回这个错误，而是EPROTO，同时完成三次握手的连接会从已完成队列中移除；在这种情况下，如果我们用select监听到有新
的连接完成，但之后又被从完成队列中删除，此时如果调用阻塞accept就会产生阻塞；

解决办法：
(1) 使用select监听套接字是否有完成连接的时候，总是把这个监听套接字设置为非阻塞；
(2) 在后续的accept调用中忽略以下错误，EWOULDBLOCK(Berkeley实现，客户中止连接), ECONNABORTED(posix实现，客户中止连接), EPROTO
(serv4实现，客户中止连接)和EINTR(如果有信号被捕获)；

==============================================================================
TCP是一种面向连接的协议，连接的建立和断开需要通过收发相应的分节来实现。某些时候，由于网络的故障或是一方主机的突然崩溃而另一方无法检测到，
以致始终保持着不存在的连接。下面介绍一种方法来检测这种异常断开的情况

1) 在TCP协议中提供了KEEPALIVE检测。该选项使能后，在一个TCP连接上，若指定的一段时间内没有数据交换，则自动发送分节等待对方确认。

     SO_KEEPALIVE : 该选项设置是否打开探测
     TCP_KEEPIDLE : 开始发送探测分节前等待的空闲时间
     TCP_KEEPINTVL: 两次发送探测分节的时间间隔
     TCP_KEEPCNT: 判定断开前发送探测分节的次数

2) 设定探测相关选项值

     int keepalive = 1;             // 打开探测
             int keepidle = 60;        // 开始探测前的空闲等待时间
             int keepintvl = 10;        // 发送探测分节的时间间隔
             int keepcnt = 3;        // 发送探测分节的次数

3) 设置套接字的属性

     if (setsockopt(sockfd, SOL_SOCKET, SO_KEEPALIVE, (void *)&keepalive, sizeof (keepalive) < 0)
     {
                     perror(“fail to set SO_KEEPALIVE”);
                     exit(-1);
     }
     if (setsockopt(sockfd, SOL_TCP, TCP_KEEPIDLE, (void *) &keepidle, sizeof (keepidle) < 0)
     {
                     perror(“fail to set SO_KEEPIDLE”);
                     exit(-1);
     }
     if (setsockopt(sockfd, SOL_TCP, TCP_KEEPINTVL, (void *)&keepintvl, sizeof (keepintvl) < 0)
     {
                     perror(“fail to set SO_KEEPINTVL”);
                     exit(-1);
     }
     if (setsockopt(sockfd, SOL_TCP, TCP_KEEPCNT, (void *)&keepcnt, sizeof (keepcnt) < 0)
     {
                     perror(“fail to set SO_KEEPALIVE”);
                     exit(-1);
     }

一旦打开KEEPALIVE探测，当TCP连接异常断开后，对sockfd进行recv操作会返回-1，并且errno的值为ETIMEDOUT。

这样一来就可以很方便的在应用程序中检测TCP连接的情况，如果检测到异常断开最简单的处理就是关闭连接。

==============================================================================

backlog参数
https://www.cnblogs.com/Orgliny/p/5780796.html
http://www.cnxct.com/something-about-phpfpm-s-backlog/

==============================================================================

拥塞控制：慢启动，拥塞避免，快重传，快恢复

流量控制：滑动窗口

重传机制：

IP分片重组

tun/tap设备

==============================================================================
IP头：
版本号 4bit 0100(ipv4) 0110(ipv6)
首部长度 4bit 4字节为单位最小为5，最大为15
服务类型 8bit
数据报长度 16bit
ID号    16bit
分段标志 3bit
分段偏移 13bit
生存期TTL 8bit
协议号   8bit ICMP=1 TCP=16 UDP=7 OSPF=89
校验和   16bit
源IP    32bit
目的IP  32bit

TCP头：
源端口 16bit
目的端口 16bit
seq num 32bit
ack num 32bit
首部长度  4bit
保留位    6bit
TCP标志位 6bit
窗口大小  16bit
校验和    16bit
紧急指针  16bit
选项字段
数据字段
TCP头部最小20字节，最大60字节
TCP头部中没有包长度字段，因为TCP是流协议，没有流量边界
TCP有自己的流量拥塞控制算法，且依靠IP层分片

ARP头：
目的mac地址 6字节
源mac地址  6字节
类型      2字节 0x0800 IP报文 / 0x0806 ARP报文 / 0x8035 RARP报文
校验和     4字节
以太网最小帧长64字节

UDP头：
源端口 16bit
目的端口 16bit
用户数据报长度 16bit
校验和 16bit
数据字段
UDP头最小8字节

==============================================================================
FTP协议：
端口20 传送文件数据
端口21 传送控制指令，如连接请求等

SNMP协议：
端口161 管理进程获取SNMP代理数据
端口162 SNMP代理主动向管理进程发送数据

DNS协议：
TCP协议 端口53 DNS区域文件传送
UDP协议 端口53 域名解析服务

RIP协议：距离矢量协议
UDP 端口520 RIPv1使用广播进行邻居 RIPv2使用组播224.0.0.9进行路由表更新

OSPF协议：
协议号89, 全部路由器 224.0.0.5 DR路由器 224.0.0.6

BGP协议：
TCP 端口号179

