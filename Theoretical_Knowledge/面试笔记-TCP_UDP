tcp和udp的区别，tcp是怎么做错误处理的?
1. 基于连接vs无连接:
UDP是无连接的协议，和点对点连接之前不需要发送消息。这就是为什么，UDP更加适合消息的多播发布，从单个点向多个点传输消息。

2. 可靠性不同

3.有序性
TCP也保证了消息的有序性。该消息将以从服务器端发出的同样的顺序发送到客户端，尽管这些消息到网络的另一端时可能是无序的。TCP协议将会为你排好序。UDP不提供任何有序性或序列性的保证。数据包将以任何可能的顺序到达。

4.数据边界
TCP不保存数据的边界，而UDP保证。在传输控制协议，数据以字节流的形式发送，并没有明显的标志表明传输信号消息（段）的边界。

5.速度
TCP速度比较慢，而UDP速度比较快，因为TCP必须创建连接，以保证消息的可靠交付和有序性，他需要做比UDP多的多的事。

6.重量级vs轻量级
TCP被认为是重量级的协议，而与之相比，UDP协议则是一个轻量级的协议。因为UDP传输的信息中不承担任何间接创造连接，保证交货或秩序的的信息。这也反映在用于承载元数据的头的大小。

7. 头大小
TCP具有比UDP更大的头。一个TCP数据包报头的大小是20字节，UDP数据报报头是8个字节。TCP报头中包含序列号，ACK号，数据偏移量，保留，控制位，窗口，紧急指针，可选项，填充项，校验位，源端口和目的端口。而UDP报头只包含长度，源端口号，目的端口，和校验和

8. 拥塞或流控制
TCP有流量控制。在任何用户数据可以被发送之前，TCP需要三数据包来设置一个套接字连接。TCP处理的可靠性和拥塞控制。另一方面，UDP不能进行流量控制。

==============================================================================

流量控制和拥塞控制的实现机制：
1）TCP采用大小可变的滑动窗口机制实现流量控制功能。窗口的大小是字节。在TCP报文段首部的窗口字段写入的数值就是当前给对方设置发送窗口的数据的上限。
在数据传输过程中，TCP提供了一种基于滑动窗口协议的流量控制机制，用接收端接收能力（缓冲区的容量）的大小来控制发送端发送的数据量。
2）采用滑动窗口机制还可对网络进行拥塞控制，将网络中的分组（TCP报文段作为其数据部分）数量维持在一定的数量之下，当超过该数值时，网络的性能会急剧恶化。
传输层的拥塞控制有慢启动（Slow-Start）、拥塞避免（Congestion Avoidance）、快重传（Fast Retransmit）和快恢复（Fast Recovery）四种算法。
拥塞：　大量数据报涌入同一交换节点（如路由器），导致该节点资源耗尽而必须丢弃后面到达的数据报时，就是拥塞。

==============================================================================

重传机制：
TCP每发送一个报文段，就设置一次定时器。只要定时器设置的重发时间到而还没有收到确认，就要重发这一报文段。 
TCP环境
报文往返时间不定、有很大差别
A、B在一个局域网络，往返时延很小
A、C在一个互联网内，往返时延很大
因此，A很难确定一个固定的、与B、C通信都适用的定时器时间
TCP采用了一种自适应算法。这种算法记录每一个报文段发出的时间，以及收到相应的确认报文段的时间。这两个时间之差就是报文段的往返时延。将各个报文段的往返时延样本加权平均，就得出报文段的平均往返时延T。

==============================================================================

滑动窗口机制：
TCP 采用大小可变的滑动窗口进行流量控制。窗口大小的单位是字节。
在 TCP 报文段首部的窗口字段写入的数值就是当前给对方设置的发送窗口数值的上限。发送窗口在连接建立时由双方商定。但在通信的过程中，接收端可根据自己的资源情况，随时动态地调整对方的发送窗口上限值(可增大或减小)。

==============================================================================

TCP的优势
从传输数据来讲，TCP/UDP以及其他协议都可以完成数据的传输，从一端传输到另外一端，TCP比较出众的一点就是提供一个可靠的，流控的数据传输，所以实现起来要比其他协议复杂的多，先来看下这两个修饰词的意义：
 1. Reliability ，提供TCP的可靠性，TCP的传输要保证数据能够准确到达目的地，如果不能，需要能检测出来并且重新发送数据。
 2. Data Flow Control，提供TCP的流控特性，管理发送数据的速率，不要超过设备的承载能力
为了能够实现以上2点，TCP实现了很多细节的功能来保证数据传输，比如说 滑动窗口适应系统，超时重传机制，累计ACK等，这次先介绍一下滑动窗口的一些知识点。

IP层协议属于不可靠的协议，IP层并不关系数据是否发送到了对端，TCP通过确认机制来保证数据传输的可靠性，在比较早的时候使用的是send--wait--send的模式，其实这种模式叫做stop-wait模式，发送数据方在发送数据之后会启动定时器，但是如果数据或者ACK丢失，那么定时器到期之后，收不到ACK就认为发送出现状况，要进行重传。这样就会降低了通信的效率，如下图所示，这种方式被称为 positive acknowledgment with retransmission (PAR)

滑动窗口
可以假设一下，来优化一下PAR效率低的缺点，比如我让发送的每一个包都有一个id，接收端必须对每一个包进行确认，这样设备A一次多发送几个片段，而不必等候ACK，同时接收端也要告知它能够收多少，这样发送端发起来也有个限制，当然还需要保证顺序性，不要乱序，对于乱序的状况，我们可以允许等待一定情况下的乱序，比如说先缓存提前到的数据，然后去等待需要的数据，如果一定时间没来就DROP掉，来保证顺序性！

在TCP/IP协议栈中，滑动窗口的引入可以解决此问题，先来看从概念上数据分为哪些类
1. Sent and Acknowledged：这些数据表示已经发送成功并已经被确认的数据，比如图中的前31个bytes，这些数据其实的位置是在窗口之外了，因为窗口内顺序最低的被确认之后，要移除窗口，实际上是窗口进行合拢，同时打开接收新的带发送的数据
2. Send But Not Yet Acknowledged：这部分数据称为发送但没有被确认，数据被发送出去，没有收到接收端的ACK，认为并没有完成发送，这个属于窗口内的数据。
3. Not Sent，Recipient Ready to Receive：这部分是尽快发送的数据，这部分数据已经被加载到缓存中，也就是窗口中了，等待发送，其实这个窗口是完全有接收方告知的，接收方告知还是能够接受这些包，所以发送方需要尽快的发送这些包
4. Not Sent，Recipient Not Ready to Receive： 这些数据属于未发送，同时接收端也不允许发送的，因为这些数据已经超出了发送端所接收的范围

对于接收端也是有一个接收窗口的，类似发送端，接收端的数据有3个分类，因为接收端并不需要等待ACK所以它没有类似的接收并确认了的分类，情况如下
1.  Received and ACK Not Send to Process：这部分数据属于接收了数据但是还没有被上层的应用程序接收，也是被缓存在窗口内
2.  Received  Not ACK: 已经接收并，但是还没有回复ACK，这些包可能输属于Delay ACK的范畴了
3.  Not Received：有空位，还没有被接收的数据。

发送窗口和可用窗口
对于发送方来讲，窗口内的包括两部分，就是发送窗口（已经发送了，但是没有收到ACK），可用窗口，接收端允许发送但是没有发送的那部分称为可用窗口。
1. Send Window ： 20个bytes 这部分值是有接收方在三次握手的时候进行通告的，同时在接收过程中也不断的通告可以发送的窗口大小，来进行适应
2. Window Already Sent: 已经发送的数据，但是并没有收到ACK。

滑动窗口原理
TCP并不是每一个报文段都会回复ACK的，可能会对两个报文段发送一个ACK，也可能会对多个报文段发送1个ACK【累计ACK】，比如说发送方有1/2/3 3个报文段，先发送了2,3 两个报文段，但是接收方期望收到1报文段，这个时候2,3报文段就只能放在缓存中等待报文1的空洞被填上，如果报文1，一直不来，报文2/3也将被丢弃，如果报文1来了，那么会发送一个ACK对这3个报文进行一次确认。
举一个例子来说明一下滑动窗口的原理：
1. 假设32~45 这些数据，是上层Application发送给TCP的，TCP将其分成四个Segment来发往internet
2. seg1 32~34 seg3 35~36 seg3 37~41 seg4 42~45  这四个片段，依次发送出去，此时假设接收端之接收到了seg1 seg2 seg4
3. 此时接收端的行为是回复一个ACK包说明已经接收到了32~36的数据，并将seg4进行缓存（保证顺序，产生一个保存seg3 的hole）
4. 发送端收到ACK之后，就会将32~36的数据包从发送并没有确认切到发送已经确认，提出窗口，这个时候窗口向右移动
5. 假设接收端通告的Window Size仍然不变，此时窗口右移，产生一些新的空位，这些是接收端允许发送的范畴
6. 对于丢失的seg3，如果超过一定时间，TCP就会重新传送（重传机制），重传成功会seg3 seg4一块被确认，不成功，seg4也将被丢弃

就是不断重复着上述的过程，随着窗口不断滑动，将真个数据流发送到接收端，实际上接收端的Window Size通告也是会变化的，接收端根据这个值来确定何时及发送多少数据，从对数据流进行流控。

滑动窗口动态调整
主要是根据接收端的接收情况，动态去调整Window Size，然后来控制发送端的数据流量
1. 客户端不断快速发送数据，服务器接收相对较慢，看下实验的结果
a. 包175，发送ACK携带WIN = 384，告知客户端，现在只能接收384个字节
b. 包176，客户端果真只发送了384个字节，Wireshark也比较智能，也宣告TCP Window Full
c. 包177，服务器回复一个ACK，并通告窗口为0，说明接收方已经收到所有数据，并保存到缓冲区，但是这个时候应用程序并没有接收这些数据，导致缓冲区没有更多的空间，故通告窗口为0, 这也就是所谓的零窗口，零窗口期间，发送方停止发送数据
d. 客户端察觉到窗口为0，则不再发送数据给接收方
e. 包178，接收方发送一个窗口通告，告知发送方已经有接收数据的能力了，可以发送数据包了
f.  包179，收到窗口通告之后，就发送缓冲区内的数据了.

==============================================================================

TIME_WAIT状态产生场景\理由\如何避免:

1. 主动关闭的Socket端会进入TIME_WAIT状态，并且持续2MSL时间长度，MSL就是maximum segment lifetime(最大分节生命期）；
这是一个IP数据包能在互联网上生存的最长时间，超过这个时间将在网络中消失。
MSL在RFC 1122上建议是2分钟，而源自berkeley的TCP实现传统上使用30秒，因而，TIME_WAIT状态一般维持在1-4分钟。

2. 主动关闭的一方在发送最后一个ack 后就会进入TIME_WAIT 状态 停留2MSL（max segment lifetime）时间这个是TCP/IP必不可少的，也就是“解决”不了的。

3. TIME_WAIT状态存在的理由：
    1）防止上一次连接中的包，迷路后重新出现，影响新连接
    （经过2MSL，上一次连接中所有的重复包都会消失）
    2）可靠的关闭TCP连接
        在进行关闭连接四路握手协议时，最后的ACK是由主动关闭端发出的，如果这个最终的ACK丢失，服务器将重发最终的FIN，因此客户端必须维护
状态信息允 许它重发最终的ACK。
如果不维持这个状态信息，那么客户端将响应RST分节，服务器将此分节解释成一个错误（在java中会抛出connection reset的SocketException)。
因而，要实现TCP全双工连接的正常终止，必须处理终止序列四个分节中任何一个分节的丢失情况，主动关闭 的客户端必须维持状态信息进入TIME_WAIT状态

4.time_wait状态如何避免
首先服务器可以设置SO_REUSEADDR套接字选项来通知内核，如果端口忙，当TCP连接位于TIME_WAIT状态时可以重用端口。在一个非常有用的场景就是，
如果你的服务器程序停止后想立即重启，而新的套接字依旧希望使用同一端口，此时SO_REUSEADDR选项就可以避免TIME_WAIT状态。

==============================================================================

tcp连接建立的时候3次握手的具体过程，以及每一步原因：

（1）第一步：源主机A的TCP向主机B发出连接请求报文段，其首部中的SYN(同步)标志位应置为1，表示想与目标主机B进行通信，并发送一个同步序列号X
(例：SEQ=100)进行同步，表明在后面传送数据时的第一个数据字节的序号是X＋1（即101）。SYN同步报文会指明客户端使用的端口以及TCP连接的初始序
号。

（2）第二步：目标主机B的TCP收到连接请求报文段后，如同意，则发回确认。在确认报中应将ACK位和SYN位置1，表示客户端的请求被接受。确认号应为X
＋1(图中为101)，同时也为自己选择一个序号Y。

（3）第三步：源主机A的TCP收到目标主机B的确认后要向目标主机B给出确认，其ACK置1，确认号为Y＋1，而自己的序号为X＋1。TCP的标准规定，SYN置
1的报文段要消耗掉一个序号。

运行客户进程的源主机A的TCP通知上层应用进程，连接已经建立。当源主机A向目标主机B发送第一个数据报文段时，其序号仍为X＋1，因为前一个确认报文段并不消耗序号。
当运行服务进程的目标主机B的TCP收到源主机A的确认后，也通知其上层应用进程，连接已经建立。至此建立了一个全双工的连接。

==============================================================================

TCP初始化序列号不能设置为一个固定值，因为这样容易被攻击者猜出后续序列号，从而遭到攻击。

RFC1948中提出了一个较好的初始化序列号ISN随机生成算法。

ISN = M + F(localhost, localport, remotehost, remoteport).

M是一个计时器，这个计时器每隔4毫秒加1。

F是一个Hash算法，根据源IP、目的IP、源端口、目的端口生成一个随机数值。
要保证hash算法不能被外部轻易推算得出，用MD5算法是一个比较好的选择。

==============================================================================

tcp断开连接的具体过程，其中每一步是为什么那么做：

1)第一步：源主机A的应用进程先向其TCP发出连接释放请求，并且不再发送数据。TCP通知对方要释放从A到B这个方向的连接，将发往主机B的TCP报文段首部的终止比特FIN置1，其序号X等于前面已传送过的数据的最后一个字节的序号加1。

2)第二步：目标主机B的TCP收到释放连接通知后即发出确认，其序号为Y，确认号为X＋1，同时通知高层应用进程，这样，从A到B的连接就释放了，连接处于半关闭状态，相当于主机A向主机B说：“我已经没有数据要发送了。但如果还发送数据，我仍接收。”此后，主机B不再接收主机A发来的数据。但若主机B还有一些数据要发送主机A，则可以继续发送。主机A只要正确收到数据，仍应向主机B发送确认。

3)第三步：若主机B不再向主机A发送数据，其应用进程就通知TCP释放连接。主机B发出的连接释放报文段必须将终止比特FIN和确认比特ACK置1，并使其序号仍为Y，但还必须重复上次已发送过的ACK＝X＋1。

4) 第四步：主机A必须对此发出确认，将ACK置1，ACK＝Y＋1，而自己的序号是X＋1。这样才把从B到A的反方向的连接释放掉。主机A的TCP再向其应用进程报告，整个连接已经全部释放。

==============================================================================

tcp建立连接和断开连接的各种过程中的状态转换细节：

客户端：主动打开SYN_SENT--->ESTABLISHED--->主动关闭FIN_WAIT_1--->FIN_WAIT_2--->TIME_WAIT

服务器端：LISTEN（被动打开）--->SYN_RCVD--->ESTABLISHED--->CLOSE_WAIT(被动关闭)--->LAST_ACK--->CLOSED

其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。

==============================================================================

syn  flood是一种常见的DOS（denial of service拒绝服务）和Ddos(distributed denial of serivce 分布式拒绝服务）攻击方式。这是一种使用TCP协议缺陷，发送大量的伪造的TCP连接请求，使得被攻击方cpu或内存资源耗尽，最终导致被攻击方无法提供正常的服务。

　　要明白这种攻击原理，还要从TCP连接的建立说起：

　　大家都知道，TCP和UDP不同，它提供一种基于连接的，可靠的字节流服务。想要通信的双方，都要首先建立一条TCP连接。这条连接的两端只有通信的双方。TCP连接的建立是这样的：
　　首先，请求端（发送端）会发一个带有SYN标志位的报文，该报文中含有发送端的初始序号ISN（initinal sequence number)和发送端使用的端口号，该报文就是请求建立连接，
　　其次，服务器收到这个请求报文后，就会回一个SYN+ACK的报文，同时这个报文中也包含服务器的ISN以及对请求端的确认序号，这个确认序号的值是请求端的序号值+1，表示请求端的请求被接受，

　　最后，请求端收到这个报文后，就会回应给服务器一个ACK报文，到此一个TCP连接就建立了。

　　上面也就是典型的TCP三次握手过程（Three-way  Handshake)。问题就是在这最后一次的确认里，如果请求端由于某种异常（死机或掉线），服务器没有收到请求端发送的回应ACK。那么第三次握手没有完成，服务器就会向请求端再次发送一个SYN+ACK报文，并等待一段时间后丢弃这个未完成的连接。这个时间长度称为SYN Timeout，一般来说是分钟的数量级（大约30秒到2分钟）；一个用户出现异常导致服务器等待一分钟是没有什么问题的。如果有恶意攻击者采用这种方式，控制大量的肉鸡来模拟这种情况，服务器端就要去维护一个大量的半连接表而消耗大量的cpu和内存资源。服务器会对这个半连接表进行一个遍历，然后尝试发送SYN+ACK来继续TCP连接的建立。实际上如果客户的TCP协议栈如果不够强大，最后的结果是服务器堆栈溢出崩溃。即使服务器端足够的强大，服务器也会因为忙于处理攻击者的TCP连接请求而无瑕理会正常的客户的请求，此时从客户端来看，服务器就已经失去响应，这时我们称做服务器遭受了SYN Flood攻击。

==============================================================================
send函数缓存问题
send()函数默认情况下会使用Nagle算法。Nagle算法通过将未确认的数据存入缓冲区直到积攒到一定数量一起发送的方法。来降低主机发送零碎小数据包的数目。所以假设send()函数发送数据过快的话，该算法会将一些数据打包后统一发出去。假设不了解这样的情况，接收端採会遇到看似非常奇怪的问题，比方成功recv()的次数与成功send()的次数不相等。在这中情况下，接收端能够通过recv()的返回值是否为0来推断发送端是否发送完成。

==============================================================================
Nagle算法：
是为了减少广域网的小分组数目，从而减小网络拥塞的出现；该算法要求一个tcp连接上最多只能有一个未被确认的未完成的小分组，在该分组ack到达之前不能发送其他的小分组，tcp需要收集这些少量的分组，并在ack到来时以一个分组的方式发送出去；其中小分组的定义是小于MSS的任何分组；该算法的优之处在于它是自适应的，确认到达的越快，数据也就发送的越快；而在希望减少微小分组数目的低速广域网上，则会发送更少的分组；

设计规则如下：

　　（1）如果包长度达到最大报文长度（MSS，Maximum Segment Size），则允许发送；
　　（2）如果该包含有FIN，则允许发送；
　　（3）设置了TCP_NODELAY选项，则允许发送；
　　（4）未设置TCP_CORK选项时，若所有发出去的小数据包（包长度小于MSS）均被确认，则允许发送；
　　（5）上述条件都未满足，但发生了超时（一般为200ms），则立即发送。

关闭Nagle：
setsockopt(fd,IPPROTO_TCP,TCP_NODELAY,(char*)&flag,sizeof(flag));

==============================================================================
延迟ACK：
如果tcp对每个数据包都发送一个ack确认，那么只是一个单独的数据包为了发送一个ack代价比较高，所以tcp会延迟一段时间，如果这段时间内有数据发送到对端，则捎带发送ack，如果在延迟ack定时器触发时候，发现ack尚未发送，则立即单独发送；
延迟ACK好处：
(1) 避免糊涂窗口综合症；
(2) 发送数据的时候将ack捎带发送，不必单独发送ack；
(3) 如果延迟时间内有多个数据段到达，那么允许协议栈发送一个ack确认多个报文段；

==============================================================================

糊涂窗体综合症
当发送应用程序产生数据的速度非常慢，或接收应用程序消耗数据的速度非常慢，或者两者都有，都会使得发送数据的报文段非常小，使得网络效率非常低，这个问题称为糊涂窗体综合症SWS(silly window syndrome)。

发送方产生的症状
假设发送方应用程序产生数据的速度非常慢。比如一次仅仅产生1B，那么就有可能产生糊涂窗体综合症。解决办法是防止发送TCP一次仅仅发送1B。必须让发送TCP等待，并把数据收集成较大的数据块后再发送。

==============================================================================
当Nagle遇上延迟ACK：
试想如下典型操作，写-写-读，即通过多个写小片数据向对端发送单个逻辑的操作，两次写数据长度小于MSS，当第一次写数据到达对端后，对端延迟ack，不发送ack，而本端因为要发送的数据长度小于MSS，所以nagle算法起作用，数据并不会立即发送，而是等待对端发送的第一次数据确认ack；这样的情况下，需要等待对端超时发送ack，然后本段才能发送第二次写的数据，从而造成延迟；

由于有Nagle算法，如果发送端启用了Nagle算法，接收端启用了TCP Delayed Acknowledge。当发送端发起两次写一次读的时候，第一次写，由于TCP没有等待ACK，直接发出去了，而第二次写的时候，第一次写的ACK还没有接收到，从而等待；而接收端有Delayed Acknowledge机制，会等待40ms以提供合并多个ACK的机会。Nagle算法的使用在一些实时性要求比较高的场合，会引起一些问题。比如项目中设计的UI鼠标远程控制远端的机器时，发现远端的鼠标操作很卡顿，这是因为鼠标消息的发送端由于Nagle算法的默认开启，是有延迟的，

==============================================================================
如下场景考虑关闭Nagle算法：
(1) 对端不向本端发送数据，并且对延时比较敏感的操作；这种操作没法捎带ack；
(2) 如上写-写-读操作；对于此种情况，优先使用其他方式，而不是关闭Nagle算法：
--使用writev，而不是两次调用write，单个writev调用会使tcp输出一次而不是两次，只产生一个tcp分节，这是首选方法；
--把两次写操作的数据复制到单个缓冲区，然后对缓冲区调用一次write；
--关闭Nagle算法，调用write两次；有损于网络，通常不考虑；
setsockopt(fd,IPPROTO_TCP,TCP_NODELAY,(char*)&flag,sizeof(flag));

==============================================================================

1、mmap保存到实际硬盘，实际存储并没有反映到主存上。优点：储存量可以很大（多于主存）（这里一个问题，需要高手解答,会不会太多拷贝到主存里面？？？）；缺点：进程间读取和写入速度要比主存的要慢。

2、shm保存到物理存储器（主存），实际的储存量直接反映到主存上。优点，进程间访问速度（读写）比磁盘要快；缺点，储存量不能非常大（多于主存）

使用上看：如果分配的存储量不大，那么使用shm；如果存储量大，那么使用shm。

==============================================================================

epoll与select的区别：

问题的引出，当需要读两个以上的I/O的时候，如果使用阻塞式的I/O，那么可能长时间的阻塞在一个描述符上面，另外的描述符虽然有数据但是不能读出来，这样实时性不能满足要求，大概的解决方案有以下几种：

1.使用多进程或者多线程，但是这种方法会造成程序的复杂，而且对与进程与线程的创建维护也需要很多的开销。（Apache服务器是用的子进程的方式，优点可以隔离用户）

2.用一个进程，但是使用非阻塞的I/O读取数据，当一个I/O不可读的时候立刻返回，检查下一个是否可读，这种形式的循环为轮询（polling），这种方法比较浪费CPU时间，因为大多数时间是不可读，但是仍花费时间不断反复执行read系统调用。

3.异步I/O（asynchronous I/O），当一个描述符准备好的时候用一个信号告诉进程，但是由于信号个数有限，多个描述符时不适用。

4.一种较好的方式为I/O多路转接（I/O multiplexing）（貌似也翻译多路复用），先构造一张有关描述符的列表（epoll中为队列），然后调用一个函数，直到这些描述符中的一个准备好时才返回，返回时告诉进程哪些I/O就绪。select和epoll这两个机制都是多路I/O机制的解决方案，select为POSIX标准中的，而epoll为Linux所特有的。

区别（epoll相对select优点）主要有三：

1.select的句柄数目受限，在linux/posix_types.h头文件有这样的声明：#define __FD_SETSIZE    1024  表示select最多同时监听1024个fd。而epoll没有，它的限制是最大的打开文件句柄数目。

2.epoll的最大好处是不会随着FD的数目增长而降低效率，在selec中采用轮询处理，其中的数据结构类似一个数组的数据结构，而epoll是维护一个队列，直接看队列是不是空就可以了。epoll只会对"活跃"的socket进行操作---这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。那么，只有"活跃"的socket才会主动的去调用 callback函数（把这个句柄加入队列），其他idle状态句柄则不会，在这点上，epoll实现了一个"伪"AIO。但是如果绝大部分的I/O都是“活跃的”，每个I/O端口使用率很高的话，epoll效率不一定比select高（可能是要维护队列复杂）。

3.使用mmap加速内核与用户空间的消息传递。无论是select,poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存拷贝就很重要，在这点上，epoll是通过内核于用户空间mmap同一块内存实现的

==============================================================================
epoll既然是对select和poll的改进，就应该能避免上述的三个缺点。那epoll都是怎么解决的呢？在此之前，我们先看一下epoll和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。

　　对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。

　　对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。

　　对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。

==============================================================================

select的几大缺点：
（1）每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
（2）同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
（3）select支持的文件描述符数量太小了，默认是1024

poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，其他的都差不多。
==============================================================================
epoll中et和lt的区别与实现原理：

epoll有2种工作方式:LT和ET。
LT(level triggered 水平触发)是缺省的工作方式，并且同时支持block和no-block socket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表。

ET (edge-triggered 边缘触发)是高速工作方式，只支持no-block socket。
在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述 符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致 了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once),不过在TCP协议中，ET模式的加速效用仍需要更多的benchmark确认。

epoll只有epoll_create,epoll_ctl,epoll_wait 3个系统调用。

==============================================================================

水平触发LT
1. 对于读操作
只要缓冲内容不为空，LT模式返回读就绪。

2. 对于写操作
只要缓冲区还不满，LT模式会返回写就绪。

边缘触发ET
1. 对于读操作
（1）当缓冲区由不可读变为可读的时候，即缓冲区由空变为不空的时候。

（2）当有新数据到达时，即缓冲区中的待读数据变多的时候。

（3）当缓冲区有数据可读，且应用进程对相应的描述符进行EPOLL_CTL_MOD 修改EPOLLIN事件时。

2. 对于写操作
（1）当缓冲区由不可写变为可写时。

（2）当有旧数据被发送走，即缓冲区中的内容变少的时候。

（3）当缓冲区有空间可写，且应用进程对相应的描述符进行EPOLL_CTL_MOD 修改EPOLLOUT事件时。
==============================================================================

epoll高效，是因为内部用了一个红黑树记录添加的socket，用了一个双向链表接收内核触发的事件。是系统级别的支持的
每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。
这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插入时间效率是lgn，其中n为树的高度)。
而所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。
这个回调方法在内核中叫ep_poll_callback,它会将发生的事件添加到rdlist双链表中。

在epoll中，对于每一个事件，都会建立一个epitem结构体

1. 不用重复传递。我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。

2. 在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。
epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket，这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层，简单的说，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。

极其高效的原因：
这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。

这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。（注：好好理解这句话！）

从上面这句可以看出，epoll的基础就是回调呀！ 

如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。

ET模式为什么要设置在非阻塞模式下工作
   因为ET模式下的读写需要一直读或写直到出错（对于读，当读到的实际字节数小于请求字节数时就可以停止），而如果你的文件描述符如果不是非阻塞的，那这个一直读或一直写势必会在最后一次阻塞。这样就不能在阻塞在epoll_wait上了，造成其他文件描述符的任务饿死。

ET模式下的读写问题，我们必须实现：
a. 对于读，只要buffer中还有数据就一直读；
b. 对于写，只要buffer还有空间且用户请求写的数据还未写完，就一直写。

==============================================================================
这个方法的原理我们在之前讨论过：当buffer中有数据可读（即buffer不空）且用户对相应fd进行epoll_mod IN事件时ET模式返回读就绪，当buffer中有可写空间（即buffer不满）且用户对相应fd进行epoll_mod OUT事件时返回写就绪。

    for (;;) {

        nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);
        if (nfds == -1) {
            perror("epoll_pwait");
            exit(EXIT_FAILURE);
        }

        for (i = 0; i < nfds; ++i) {

            fd = events[i].data.fd;

            if (fd == listenfd) {

                while ((conn_sock = accept(listenfd,(struct sockaddr *) &remote, 
                                (size_t *)&addrlen)) > 0) {

                    setnonblocking(conn_sock);//下面设置ET模式，所以要设置非阻塞
                    ev.events = EPOLLIN | EPOLLET;
                    ev.data.fd = conn_sock;
                    if (epoll_ctl(epfd, EPOLL_CTL_ADD, conn_sock, &ev) == -1) {//读监听
                        perror("epoll_ctl: add"); //连接套接字
                        exit(EXIT_FAILURE);
                    }
                }
                if (conn_sock == -1) {
                    if (errno != EAGAIN && errno != ECONNABORTED 
                            && errno != EPROTO && errno != EINTR) 
                        perror("accept");
                }
                continue;
            }

            if (events[i].events & EPOLLIN) {
                n = 0;
                while ((nread = read(fd, buf + n, BUFSIZ-1)) > 0) {//ET下可以读就一直读
                    n += nread;
                }
                if (nread == -1 && errno != EAGAIN) {
                    perror("read error");
                }
                ev.data.fd = fd;
                ev.events = events[i].events | EPOLLOUT; //MOD OUT 
                if (epoll_ctl(epfd, EPOLL_CTL_MOD, fd, &ev) == -1) {
                    perror("epoll_ctl: mod");
                }
            }
            
            if (events[i].events & EPOLLOUT) {
              sprintf(buf, "HTTP/1.1 200 OK\r\nContent-Length: %d\r\n\r\nHello World", 11);
                int nwrite, data_size = strlen(buf);
                n = data_size;
                while (n > 0) {
                    nwrite = write(fd, buf + data_size - n, n);//ET下一直将要写数据写完
                    if (nwrite < n) {
                        if (nwrite == -1 && errno != EAGAIN) {
                            perror("write error");
                        }
                        break;
                    }
                    n -= nwrite;
                }
                close(fd);
            }
        }
    }

==============================================================================

一道腾讯后台开发的面试题

使用Linux epoll模型，水平（LT）触发模式，当socket可写时，会不停的触发socket可写的事件，如何处理？

第一种最普遍的方式：
需要向socket写数据的时候才把socket加入epoll，等待可写事件。接受到可写事件后，调用write或者send发送数据。当所有数据都写完后，把socket移出epoll。

这种方式的缺点是，即使发送很少的数据，也要把socket加入epoll，写完后在移出epoll，有一定操作代价。

一种改进的方式：
开始不把socket加入epoll，需要向socket写数据的时候，直接调用write或者send发送数据。如果返回EAGAIN，把socket加入epoll，在epoll的驱动下写数据，全部数据发送完毕后，再移出epoll。

这种方式的优点是：数据不多的时候可以避免epoll的事件处理，提高效率。

==============================================================================

单机最大tcp连接数

在tcp应用中，server事先在某个固定端口监听，client主动发起连接，经过三路握手后建立tcp连接。那么对单机，其最大并发tcp连接数是多少？如何标识一个TCP连接

在确定最大连接数之前，先来看看系统如何标识一个tcp连接。系统用一个4四元组来唯一标识一个TCP连接：{local ip, local port,remote ip,remote port}。

client最大tcp连接数

client每次发起tcp连接请求时，除非绑定端口，通常会让系统选取一个空闲的本地端口（local port），该端口是独占的，不能和其他tcp连接共享。tcp端口的数据类型是unsigned short，因此本地端口个数最大只有65536，端口0有特殊含义，不能使用，这样可用端口最多只有65535，所以在全部作为client端的情况下，最大tcp连接数为65535，这些连接可以连到不同的server ip。

server最大tcp连接数

server通常固定在某个本地端口上监听，等待client的连接请求。不考虑地址重用（unix的SO_REUSEADDR选项）的情况下，即使server端有多个ip，本地监听端口也是独占的，因此server端tcp连接4元组中只有remote ip（也就是client ip）和remote port（客户端port）是可变的，因此最大tcp连接为客户端ip数×客户端port数，对IPV4，不考虑ip地址分类等因素，最大tcp连接数约为2的32次方（ip数）×2的16次方（port数），也就是server端单机最大tcp连接数约为2的48次方。

实际的tcp连接数

上面给出的是理论上的单机最大连接数，在实际环境中，受到机器资源、操作系统等的限制，特别是sever端，其最大并发tcp连接数远不能达到理论上限。在unix/linux下限制连接数的主要因素是内存和允许的文件描述符个数（每个tcp连接都要占用一定内存，每个socket就是一个文件描述符），另外1024以下的端口通常为保留端口。在默认2.6内核配置下，经过试验，每个socket占用内存在15~20k之间。

影响一个socket占用内存的参数包括：

rmem_max

wmem_max

tcp_rmem

tcp_wmem

tcp_mem

grep skbuff /proc/slabinfo

对server端，通过增加内存、修改最大文件描述符个数等参数，单机最大并发TCP连接数超过10万 是没问题的

==============================================================================

https://blog.csdn.net/russell_tao/article/details/18711023

linux上还提供了以下系统级的配置来整体设置服务器上的TCP内存使用

sysctl -a
net.ipv4.tcp_rmem = 8192 87380 16777216  
net.ipv4.tcp_wmem = 8192 65536 16777216  
net.ipv4.tcp_mem = 8388608 12582912 16777216  
net.core.rmem_default = 262144  
net.core.wmem_default = 262144  
net.core.rmem_max = 16777216  
net.core.wmem_max = 16777216

net.ipv4.tcp_moderate_rcvbuf = 1  
net.ipv4.tcp_adv_win_scale = 2

应用程序编程时可以设置setsockopt()、SO_SNDBUF、SO_RCVBUF

==============================================================================

先从应用程序编程时可以设置的SO_SNDBUF、SO_RCVBUF说起

无论何种语言，都对TCP连接提供基于setsockopt方法实现的SO_SNDBUF、SO_RCVBUF，怎么理解这两个属性的意义呢？SO_SNDBUF、SO_RCVBUF都是个体化的设置，即，只会影响到设置过的连接，而不会对其他连接生效。SO_SNDBUF表示这个连接上的内核写缓存上限。实际上，进程设置的SO_SNDBUF也并不是真的上限，在内核中会把这个值翻一倍再作为写缓存上限使用，我们不需要纠结这种细节，只需要知道，当设置了SO_SNDBUF时，就相当于划定了所操作的TCP连接上的写缓存能够使用的最大内存。然而，这个值也不是可以由着进程随意设置的，它会受制于系统级的上下限，当它大于上面的系统配置wmem_max（net.core.wmem_max）时，将会被wmem_max替代（同样翻一倍）；而当它特别小时，例如在2.6.18内核中设计的写缓存最小值为2K字节，此时也会被直接替代为2K。

SO_RCVBUF表示连接上的读缓存上限，与SO_SNDBUF类似，它也受制于rmem_max配置项，实际在内核中也是2倍大小作为读缓存的使用上限。SO_RCVBUF设置时也有下限，同样在2.6.18内核中若这个值小于256字节就会被256所替代。

==============================================================================

可以设置的SO_SNDBUF、SO_RCVBUF缓存使用上限与实际内存到底有怎样的关系呢？

TCP连接所用内存主要由读写缓存决定，而读写缓存的大小只与实际使用场景有关，在实际使用未达到上限时，SO_SNDBUF、SO_RCVBUF是不起任何作用的。对读缓存来说，接收到一个来自连接对端的TCP报文时，会导致读缓存增加，当然，如果加上报文大小后读缓存已经超过了读缓存上限，那么这个报文会被丢弃从而读缓存大小维持不变。什么时候读缓存使用的内存会减少呢？当进程调用read、recv这样的方法读取TCP流时，读缓存就会减少。因此，读缓存是一个动态变化的、实际用到多少才分配多少的缓冲内存，当这个连接非常空闲时，且用户进程已经把连接上接收到的数据都消费了，那么读缓存使用内存就是0。

写缓存也是同样道理。当用户进程调用send或者write这样的方法发送TCP流时，就会造成写缓存增大。当然，如果写缓存已经到达上限，那么写缓存维持不变，向用户进程返回失败。而每当接收到TCP连接对端发来的ACK确认了报文的成功发送时，写缓存就会减少，这是因为TCP的可靠性决定的，发出去报文后由于担心报文丢失而不会销毁它，可能会由重发定时器来重发报文。因此，写缓存也是动态变化的，空闲的正常连接上，写缓存所用内存通常也为0。

因此，只有当接收网络报文的速度大于应用程序读取报文的速度时，可能使读缓存达到了上限，这时这个缓存使用上限才会起作用。所起作用为：丢弃掉新收到的报文，防止这个TCP连接消耗太多的服务器资源。同样，当应用程序发送报文的速度大于接收对方确认ACK报文的速度时，写缓存可能达到上限，从而使send这样的方法失败，内核不为其分配内存。
==============================================================================

缓存的大小与TCP的滑动窗口到底有什么关系？

读缓存的作用有2个：1、将无序的、落在接收滑动窗口内的TCP报文缓存起来；2、当有序的、可以供应用程序读取的报文出现时，由于应用程序的读取是延时的，所以会把待应用程序读取的报文也保存在读缓存中。所以，读缓存一分为二，一部分缓存无序报文，一部分缓存待延时读取的有序报文。这两部分缓存大小之和由于受制于同一个上限值，所以它们是会互相影响的，当应用程序读取速率过慢时，这块过大的应用缓存将会影响到套接字缓存，使接收滑动窗口缩小，从而通知连接的对端降低发送速度，避免无谓的网络传输。当应用程序长时间不读取数据，造成应用缓存将套接字缓存挤压到没空间，那么连接对端会收到接收窗口为0的通知，告诉对方：我现在消化不了更多的报文了。

TCP协议需要考虑复杂的网络环境，所以使用了慢启动、拥塞窗口（参见高性能网络编程2----TCP消息的发送），建立连接时的初始窗口并不会按照接收缓存的最大值来初始化。这是因为，过大的初始窗口从宏观角度，对整个网络可能造成过载引发恶性循环，也就是考虑到链路上各环节的诸多路由器、交换机可能扛不住压力不断的丢包（特别是广域网），而微观的TCP连接的双方却只按照自己的读缓存上限作为接收窗口，这样双方的发送窗口（对方的接收窗口）越大就对网络产生越坏的影响。慢启动就是使初始窗口尽量的小，随着接收到对方的有效报文，确认了网络的有效传输能力后，才开始增大接收窗口。

不同的linux内核有着不同的初始窗口。

将1500字节的MTU去除了20字节的IP头、20字节的TCP头以后，一个最大报文能够承载的有效数据长度1460。
但有些网络中，会在TCP的可选头部里，使用12字节作为时间戳使用，这样，有效数据就是MSS再减去12，初始窗口就是（1460-12）*4=5792，这与窗口想表达的含义是一致的，即：我能够处理的有效数据长度。

当窗口从初始窗口一路扩张到最大接收窗口时，最大接收窗口就是最大读缓存吗？
不是，因为必须分一部分缓存用于应用程序的延时报文读取。到底会分多少出来呢？这是可配的系统选项，如下：
net.ipv4.tcp_adv_win_scale = 2 

这里的tcp_adv_win_scale意味着，将要拿出1/(2^tcp_adv_win_scale)缓存出来做应用缓存。即，默认tcp_adv_win_scale配置为2时，就是拿出至少1/4的内存用于应用读缓存，那么，最大的接收滑动窗口的大小只能到达读缓存的3/4。

最大读缓存到底应该设置到多少为合适呢？

当应用缓存所占的份额通过tcp_adv_win_scale配置确定后，读缓存的上限应当由最大的TCP接收窗口决定。初始窗口可能只有4个或者10个MSS，但在无丢包情形下随着报文的交互窗口就会增大，当窗口过大时，“过大”是什么意思呢？即，对于通讯的两台机器的内存而言不算大，但是对于整个网络负载来说过大了，就会对网络设备引发恶性循环，不断的因为繁忙的网络设备造成丢包。而窗口过小时，就无法充分的利用网络资源。所以，一般会以BDP来设置最大接收窗口（可计算出最大读缓存）。BDP叫做带宽时延积，也就是带宽与网络时延的乘积，例如若我们的带宽为2Gbps，时延为10ms，那么带宽时延积BDP则为2G/8*0.01=2.5MB，所以这样的网络中可以设最大接收窗口为2.5MB，这样最大读缓存可以设为4/3*2.5MB=3.3MB。

为什么呢？因为BDP就表示了网络承载能力，最大接收窗口就表示了网络承载能力内可以不经确认发出的报文。

经常提及的所谓长肥网络，“长”就是是时延长，“肥”就是带宽大，这两者任何一个大时，BDP就大，都应导致最大窗口增大，进而导致读缓存上限增大。所以在长肥网络中的服务器，缓存上限都是比较大的。（当然，TCP原始的16位长度的数字表示窗口虽然有上限，但在RFC1323中定义的弹性滑动窗口使得滑动窗口可以扩展到足够大。）

发送窗口实际上就是TCP连接对方的接收窗口，所以大家可以按接收窗口来推断，这里不再啰嗦。

==============================================================================

linux的TCP缓存上限自动调整策略

对于一个TCP连接来说，可能已经充分利用网络资源，使用大窗口、大缓存来保持高速传输了。比如在长肥网络中，缓存上限可能会被设置为几十兆字节，但系统的总内存却是有限的，当每一个连接都全速飞奔使用到最大窗口时，1万个连接就会占用内存到几百G了，这就限制了高并发场景的使用，公平性也得不到保证。我们希望的场景是，在并发连接比较少时，把缓存限制放大一些，让每一个TCP连接开足马力工作；当并发连接很多时，此时系统内存资源不足，那么就把缓存限制缩小一些，使每一个TCP连接的缓存尽量的小一些，以容纳更多的连接。

linux为了实现这种场景，引入了自动调整内存分配的功能，由tcp_moderate_rcvbuf配置决定，如下：
net.ipv4.tcp_moderate_rcvbuf = 1
默认tcp_moderate_rcvbuf配置为1，表示打开了TCP内存自动调整功能。若配置为0，这个功能将不会生效（慎用）。

另外请注意：当我们在编程中对连接设置了SO_SNDBUF、SO_RCVBUF，将会使linux内核不再对这样的连接执行自动调整功能！
tcp_rmem[3]数组表示任何一个TCP连接上的读缓存上限，其中tcp_rmem[0]表示最小上限，tcp_rmem[1]表示初始上限（注意，它会覆盖适用于所有协议的rmem_default配置），tcp_rmem[2]表示最大上限。tcp_wmem[3]数组表示写缓存，与tcp_rmem[3]类似，不再赘述。

tcp_mem[3]数组就用来设定TCP内存的整体使用状况，所以它的值很大（它的单位也不是字节，而是页--4K或者8K等这样的单位！）。这3个值定义了TCP整体内存的无压力值、压力模式开启阀值、最大使用值。以这3个值为标记点则内存共有4种情况：
(1)、当TCP整体内存小于tcp_mem[0]时，表示系统内存总体无压力。若之前内存曾经超过了tcp_mem[1]使系统进入内存压力模式，那么此时也会把压力模式关闭。
这种情况下，只要TCP连接使用的缓存没有达到上限（注意，虽然初始上限是tcp_rmem[1]，但这个值是可变的，下文会详述），那么新内存的分配一定是成功的。

(2)、当TCP内存在tcp_mem[0]与tcp_mem[1]之间时，系统可能处于内存压力模式，例如总内存刚从tcp_mem[1]之上下来；也可能是在非压力模式下，例如总内存刚从tcp_mem[0]以下上来。此时，无论是否在压力模式下，只要TCP连接所用缓存未超过tcp_rmem[0]或者tcp_wmem[0]，那么都一定都能成功分配新内存。否则，基本上就会面临分配失败的状况。（注意：还有一些例外场景允许分配内存成功，由于对于我们理解这几个配置项意义不大，故略过。）

(3)、当TCP内存在tcp_mem[1]与tcp_mem[2]之间时，系统一定处于系统压力模式下。其他行为与上同。

(4)、当TCP内存在tcp_mem[2]之上时，毫无疑问，系统一定在压力模式下，而且此时所有的新TCP缓存分配都会失败。

当系统在非压力模式下，上面我所说的每个连接的读写缓存上限，才有可能增加，当然最大也不会超过tcp_rmem[2]或者tcp_wmem[2]。相反，在压力模式下，读写缓存上限则有可能减少，虽然上限可能会小于tcp_rmem[0]或者tcp_wmem[0]。

所以，粗略的总结下，对这3个数组可以这么看：
1、只要系统TCP的总体内存超了 tcp_mem[2] ，新内存分配都会失败。
2、tcp_rmem[0]或者tcp_wmem[0]优先级也很高，只要条件1不超限，那么只要连接内存小于这两个值，就保证新内存分配一定成功。
3、只要总体内存不超过tcp_mem[0]，那么新内存在不超过连接缓存的上限时也能保证分配成功。
4、tcp_mem[1]与tcp_mem[0]构成了开启、关闭内存压力模式的开关。在压力模式下，连接缓存上限可能会减少。在非压力模式下，连接缓存上限可能会增加，最多增加到tcp_rmem[2]或者tcp_wmem[2]。

==============================================================================

net.ipv4.tcp_wmem = 4096 16384 4194304
net.ipv4.tcp_rmem = 4096 87380 4194304

就是说，每个tcp连接的socket，至少需要8k字节，那么对于8G内存的机器，如果不考虑swap等其他因素，最多支持并发100万个tcp socket

每一个tcp连接占用的内存计算可以简化为：tcp写（发送）缓存+tcp读（接收）缓存。

因此，一个tcp连接在内核中需要占用的最小内存是：net.ipv4.tcp_wmem的最小值 + net.ipv4.tcp_rmem的最小值，根据默认配置，这个值就是4096+4096，也就是8k。因此，我们可以说默认情况一个tcp连接占用的内存应该是8k！

http://blog.csdn.net/fox_hacker/article/details/41440561

而对于内存，tcp连接归根结底需要双方接收和发送数据，那么就需要一个读缓冲区和写缓冲区，这两个buffer在linux下最小为4096字节，可通过
cat /proc/sys/net/ipv4/tcp_rmem和cat /proc/sys/net/ipv4/tcp_wmem来查看。所以，一个tcp连接最小占用内存为4096+4096 = 8k，

http://benpaozhe.blog.51cto.com/10239098/1752675

TCP读缓存大小，单位是字节：第一个是最小值4K，第二个是默认值85K，第三个是最大值16M，这个可以在sysctl.conf中net.ipv4.tcp_rmem中进行调整。

TCP写缓存大小，单位是字节：第一个是最小值4K，第二个是默认值64K，第三个是最大值16M， 这个可以在sysctl.conf中net.ipv4.tcp_wmem中进行调整。

也就是说一个TCP在三次握手建立连接后，最小的内存消耗在8K左右

==============================================================================

对于接收缓冲区和发送缓冲区，如果没有数据，是不占内存的。具体来说，对于接收缓冲区，只有当有数据可读但应用程序尚未读取的时候才占内存（就是 epoll_wait 返回 EPOLL_IN之后，程序调用 read() 之前的那一小段时间）。换句话说，只要服务器总是及时读取数据，接收缓冲区基本不占内存。对于发送缓冲区，只有等待发送的数据和发送之后尚未收到 ACK 的数据才占用内存，在稳态下，发送缓冲区占用的内存等于 BDP。比如考虑发送方每秒钟发 1MB 数据，rtt 是 50ms 的情况，那么发送缓冲区平均占用 51.2 KB （不计 skbuff 的额外开销）。对于千兆网环境，一台单网口的机器最多支持 112 个这样的连接，即便考虑 skbuff 的额外开销，所有这些连接的发送缓冲区一共占用不到 10MB 内存。（即便 rtt 高达 1s，那么千兆网的BDP 是 125MB，比起购买带宽的成本，这点内存开销可以忽略不计）。

服务端在收到 SYN 之后不会立刻创建 tcp_sock，而是会创建 tcp_request_sock 来处理三路握手，后者要小得多（256 字节），等到收到三路握手最后的 ACK 才创建 tcp_sock。主动关闭 TCP socket 会进入 TIME_WAIT 状态，tcp_sock 会被释放，取而代之的是小得多的 inet_timewait_sock 对象（192 字节），因此 TIME_WAIT 并不占用多少资源。总之，Linux 协议栈尽可能缩小 tcp_sock 的生命期，以节约内存。

==============================================================================
Linux如何查看端口
lsof -i:端口号 用于查看某一端口的占用情况，比如查看8000端口使用情况，lsof -i:8000
netstat -tunlp |grep 端口号，用于查看指定的端口号的进程情况，如查看8000端口的情况，netstat -tunlp |grep 8000

==============================================================================

https://www.cnblogs.com/wanpengcoder/p/5356776.html
https://www.cnblogs.com/549294286/p/5208357.html
1. connect出错：
(1) 若TCP客户端没有收到syn分节的响应，则返回ETIMEOUT错误；调用connect函数时，内核发送一个syn，若无响应则等待6s后再发送一个，若仍然无响应则等待24s后在发送一个，若总共等待75s后仍未收到响应则返回本错误；
(2) 若对客户的syn响应是rst，则表明该服务器在我们指定的端口上没有进程在等待与之连接，这是一种硬错误，客户一收到rst马上返回ECONNREFUSED错误；
(3) 若客户发送的syn在中间的某个路由器上引发了目的不可达icmp错误，则认为是一种软错误。客户主机内核保存该消息，并按照第一种情况的时间间隔继续发送syn，在某个规定时间后仍未收到响应，则把保存的消息作为EHOSTUNREACH或者ENETUNREACH错误返回给进程；

2. accept返回前连接中止：
在比较忙的服务器中，在建立三次握手之后，调用accept之前，可能出现客户端断开连接的情况；如，三次握手之后，客户端发送rst，然后服务器调用accept。posix指出这种情况errno设置为CONNABORTED;
注意Berkeley实现中，没有返回这个错误，而是EPROTO，同时完成三次握手的连接会从已完成队列中移除；在这种情况下，如果我们用select监听到有新的连接完成，但之后又被从完成队列中删除，此时如果调用阻塞accept就会产生阻塞；
解决办法：
(1) 使用select监听套接字是否有完成连接的时候，总是把这个监听套接字设置为非阻塞；
(2) 在后续的accept调用中忽略以下错误，EWOULDBLOCK(Berkeley实现，客户中止连接), ECONNABORTED(posix实现，客户中止连接), EPROTO(serv4实现，客户中止连接)和EINTR(如果有信号被捕获)；

3. 服务器进程被终止(kill)：
在客户端和服务器端建立连接之后，使用kill命令杀死服务器进程，进程终止会关闭所有打开的描述符，这导致了其向客户端发送了一个FIN，而客户端则响应了一个ack，这就完成了tcp连接终止的前半部分，只代表服务器不在发送数据了；但是客户端并不知道服务器端已经终止了，当客户端向服务器写数据的时候，由于服务器进程终止，所以响应了rst，如果我们使用select等方式，能够立即知道当前连接状态；如下：
(1) 如果对端tcp发送数据，那么套接字可读，并且read返回一个大于0的值(读入字节数)；
(2) 如果对端tcp发送了fin(对端进程终止)，那么该套接字变为可读，并且read返回0(EOF)；
(3) 如果对端tcp发送rst(对端主机崩溃并重启)，那么该套接字变为可读，并且read返回-1，errno中含有确切错误码；

4.服务器主机崩溃：
建立连接之后，服务器主机崩溃，此时如果客户端发送数据，会发现客户端会在一定时间内持续重传，视图从服务器端收到数据的ack，当重传时间超过指定时间后，服务器仍然没有响应，那么返回的是ETIMEDOUT；

5.服务器主机不可达： 
建立连接之后，服务器主机未崩溃，但是由于中间路由器故障灯，判定主机或网络不可达，此时如果客户端发送数据，会发现客户端会在一定时间内持续重传，视图从服务器端收到数据的ack，当重传时间超过指定时间后，服务器仍然没有响应，那么返回的是EHOSTUNREACH或ENETUNREACH；

6.服务器主机崩溃后重启：
当服务器主机崩溃重启后，之前所有的tcp连接丢失，此时服务器若收到来自客户端的数据，会响应一个rst；客户端调用read将返回一个ECONNRESET错误；

7.服务器主机关机：
系统关机时，init进程给所有进程发送SIGTERM信号，等待固定的时间，然后给所有仍在运行的进程发送SIGKILL信号，我们的进程会被SIGTERM或者SIGKILL信号终止，所以与前面服务器进程终止相同，进程关闭所有描述符，并发送fin，完成服务器端的半关闭；

8.sigpipe信号：
当一个进程向某个收到rst的套接字执行写操作的时候，内核向该进程发送一个SIGPIPE信号，
该信号的默认行为是终止进程，因此进程必须捕获它以免不情愿的被终止；
不论进程是捕捉了该信号并从信号处理函数中返回，还是简单忽略该信号，写操作都将返回EPIPE错误；

==============================================================================
也就是socket连接的一端如何知道连接已经被动关闭，如另一端强退，另一端close等？
网络连接socket建立后，若某一端关闭连接，而另一端仍然向它写数据，第一次写数据后会收到RST响应，
此后再写数据，kernel将向进程发出SIGPIPE信号，通知进程此连接已经断开。
而SIGPIPE信号的默认处理是终止程序，导致上述问题的发生！
SIGPIPE：Broken pipe：向一个没有读端的管道写数据。默认动作为终止进程。
但是我并不希望终止进程，而是等待write返回错误，来判断connect断开，因此我需要忽略该信号。

要在服务器端判断客户端已经关闭了连接，可以用write的返回值判断。
但是要注意的是：write只是把服务端应用层中的数据发送给TCP层发送缓存，
然后由TCP层将应用层数据再发送给客户端TCP层，所以，当客户端关闭连接时，服务器端write并不会立刻返回-1。
通过抓包工具抓包发现，客户端向服务器端发送FIN之后，服务器端TCP层还是向客户端发出了几个报文的，不过会收到客户端的RST。
当服务端TCP层收到客户端TCP层发过来的RST报文之后，就会检测到对方已经关闭了连接，这时候再去write就会出错，中间是有一段延迟的！
==============================================================================

套接字的默认状态是阻塞的，这就意味着当发出一个不能立即完成的套接字调用时，其进程将被投入睡眠，等待响应操作完成，可能阻塞的套接字调用可分为
以下四类：

(1) 输入操作，包括read，readv，recv，recvfrom，recvmsg；
(2) 输出操作，包括write，writev，send，sendto，sendmsg；
(3) 接受外来连接，即accept函数。
(4) 发起外出连接，即tcp的connect函数；

==============================================================================
非阻塞connect:
当一个非阻塞的tcp套接字上调用connect时，connect将立即返回一个EINPROGRESS错误，不过已经发起的tcp三路握手继续进行。我们接着使用
select检测这个连接或成功或失败的已建立条件。非阻塞connect有三个用途：
(1) 我们可以把三路握手叠加在其他处理上，完成一个connect要花的RTT时间，而RTT波动很大，从局域网上的几毫秒到几百毫秒甚至是广域网的几秒。这段时间内也许有我们想要执行的其他工作可执行；
(2) 我们可以使用这个技术同时建立多个连接；这个技术随着web浏览器流行起来；
(3) 既然使用select等待连接建立，我们可以给select指定一个时间限制，使得我们能够缩短connect的超时。

非阻塞connect细节：
(1) 尽管套接字是非阻塞的，如果连接到的服务器在同一个主机上，那么当我们调用connect时候，连接通常立刻建立，我们必须处理这种情形；
(2) 源自Berkeley的实现(和posix)有关select和非阻塞connect的以下两个原则:
--(a) 当连接成功建立时，描述符变为可写；
--(b) 当连接建立遇到错误时，描述符变为既可读又可写；

==============================================================================
非阻塞accept：

在比较忙的服务器中，在建立三次握手之后，调用accept之前，可能出现客户端断开连接的情况，再这样的情况下；如，三次握手之后，客户端发送rst，然后服务器调用accept。posix指出这种情况errno设置为CONNABORTED;

注意Berkeley实现中，没有返回这个错误，而是EPROTO，同时完成三次握手的连接会从已完成队列中移除；在这种情况下，如果我们用select监听到有新的连接完成，但之后又被从完成队列中删除，此时如果调用阻塞accept就会产生阻塞；

解决办法：
(1) 使用select监听套接字是否有完成连接的时候，总是把这个监听套接字设置为非阻塞；
(2) 在后续的accept调用中忽略以下错误，EWOULDBLOCK(Berkeley实现，客户中止连接), ECONNABORTED(posix实现，客户中止连接), EPROTO
(serv4实现，客户中止连接)和EINTR(如果有信号被捕获)；

==============================================================================
TCP是一种面向连接的协议，连接的建立和断开需要通过收发相应的分节来实现。某些时候，由于网络的故障或是一方主机的突然崩溃而另一方无法检测到，以致始终保持着不存在的连接。下面介绍一种方法来检测这种异常断开的情况

1) 在TCP协议中提供了KEEPALIVE检测。该选项使能后，在一个TCP连接上，若指定的一段时间内没有数据交换，则自动发送分节等待对方确认。

     SO_KEEPALIVE : 该选项设置是否打开探测
     TCP_KEEPIDLE : 开始发送探测分节前等待的空闲时间
     TCP_KEEPINTVL: 两次发送探测分节的时间间隔
     TCP_KEEPCNT: 判定断开前发送探测分节的次数

2) 设定探测相关选项值

     int keepalive = 1;             // 打开探测
             int keepidle = 60;        // 开始探测前的空闲等待时间
             int keepintvl = 10;        // 发送探测分节的时间间隔
             int keepcnt = 3;        // 发送探测分节的次数

3) 设置套接字的属性

     if (setsockopt(sockfd, SOL_SOCKET, SO_KEEPALIVE, (void *)&keepalive, sizeof (keepalive) < 0)
     {
             perror(“fail to set SO_KEEPALIVE”);
             exit(-1);
     }
     if (setsockopt(sockfd, SOL_TCP, TCP_KEEPIDLE, (void *) &keepidle, sizeof (keepidle) < 0)
     {
             perror(“fail to set SO_KEEPIDLE”);
             exit(-1);
     }
     if (setsockopt(sockfd, SOL_TCP, TCP_KEEPINTVL, (void *)&keepintvl, sizeof (keepintvl) < 0)
     {
             perror(“fail to set SO_KEEPINTVL”);
             exit(-1);
     }
     if (setsockopt(sockfd, SOL_TCP, TCP_KEEPCNT, (void *)&keepcnt, sizeof (keepcnt) < 0)
     {
             perror(“fail to set SO_KEEPALIVE”);
             exit(-1);
     }

一旦打开KEEPALIVE探测，当TCP连接异常断开后，对sockfd进行recv操作会返回-1，并且errno的值为ETIMEDOUT。

这样一来就可以很方便的在应用程序中检测TCP连接的情况，如果检测到异常断开最简单的处理就是关闭连接。

==============================================================================

backlog参数
https://www.cnblogs.com/Orgliny/p/5780796.html
http://www.cnxct.com/something-about-phpfpm-s-backlog/

TCP建立连接是要进行三次握手，但是否完成三次握手后，服务器就处理（accept）呢？

backlog其实是一个连接队列，在Linux内核2.2之前，backlog大小包括半连接状态和全连接状态两种队列大小。

半连接状态为：服务器处于Listen状态时收到客户端SYN报文时放入半连接队列中，即SYN queue（服务器端口状态为：SYN_RCVD）。

全连接状态为：TCP的连接状态从服务器（SYN+ACK）响应客户端后，到客户端的ACK报文到达服务器之前，则一直保留在半连接状态中；
当服务器接收到客户端的ACK报文后，该条目将从半连接队列搬到全连接队列尾部，即 accept queue （服务器端口状态为：ESTABLISHED）。

在Linux内核2.2之后，分离为两个backlog来分别限制半连接（SYN_RCVD状态）队列大小和全连接（ESTABLISHED状态）队列大小。

SYN queue 队列长度由 /proc/sys/net/ipv4/tcp_max_syn_backlog 指定，默认为2048。

Accept queue 队列长度由 /proc/sys/net/core/somaxconn 和使用listen函数时传入的参数，二者取最小值。默认为128。
在Linux内核2.4.25之前，是写死在代码常量 SOMAXCONN ，
在Linux内核2.4.25之后，在配置文件 /proc/sys/net/core/somaxconn 中直接修改，
或者在 /etc/sysctl.conf 中配置 net.core.somaxconn = 128 。

可以通过ss命令来显示
在LISTEN状态，其中 Send-Q 即为Accept queue的最大值，Recv-Q 则表示Accept queue中等待被服务器accept()。
另外客户端connect()返回不代表TCP连接建立成功，有可能此时accept queue 已满，系统会直接丢弃后续ACK请求；
客户端误以为连接已建立，开始调用等待至超时；
服务器则等待ACK超时，会重传SYN+ACK 给客户端，
重传次数受限 net.ipv4.tcp_synack_retries ，默认为5，表示重发5次，每次等待30~40秒，
即半连接默认时间大约为180秒，该参数可以在tcp被洪水攻击是临时启用这个参数。

查看SYN queue 溢出 netstat -s | grep LISTEN
102324 SYNs to LISTEN sockets dropped

查看Accept queue 溢出 netstat -s | grep TCPBacklogDrop
TCPBacklogDrop: 2334

==============================================================================

TCP3次握手实际上可分为4步

1 客户端发起connect()，发送SYN j

2 服务器从SYN queue中建立条目，响应SYN k, ACK J+1

3 客户端connect()成功返回，响应ACK K+1

4 服务器将socket从SYN queue移入accept queue，accept()成功返回

注：SYN/FIN各占一个序列号，ACK/RST不占序列号

客户端connect()返回不代表TCP连接建立成功，有可能此时服务器accept queue已满，OS会直接丢弃后续ACK请求；
客户端以为连接已建立，开始后续调用(譬如send)等待直至超时；
服务器则等待ACK超时，会重传SYN k, ACK J+1给客户端(重传次数受限net.ipv4.tcp_synack_retries)；
注：accept queue溢出，即便SYN queue没有溢出，新连接请求的SYN也可能被drop

==============================================================================

先来回顾下三次握手里面涉及到的问题:
1.当 client 通过 connect 向 server 发出 SYN 包时，client 会维护一个 socket 等待队列，而 server 会维护一个 SYN 队列
2.此时进入半链接的状态，如果 socket 等待队列满了，server 则会丢弃，而 client 也会由此返回 connection time out；只要是 client 没有收到 SYN+ACK，3s 之后，client 会再次发送，如果依然没有收到，9s 之后会继续发送。
3.半连接 syn 队列的长度为 max(64, /proc/sys/net/ipv4/tcp_max_syn_backlog)  决定
4.当 server 收到 client 的 SYN 包后，会返回 SYN, ACK 的包加以确认，client 的 TCP 协议栈会唤醒 socket 等待队列，发出 connect 调用
5.client 返回 ACK 的包后，server 会进入一个新的叫 accept 的队列，该队列的长度为 min(backlog, somaxconn)，默认情况下，somaxconn 的值为 128，表示最多有 129 的 ESTAB 的连接等待 accept()，而 backlog 的值则由 int listen(int sockfd, int backlog) 中的第二个参数指定，listen 里面的 backlog 的含义请看这里。需要注意的是，一些 Linux 的发型版本可能存在对 somaxcon 错误 truncating 方式。
6.当 accept 队列满了之后，即使 client 继续向 server 发送 ACK 的包，也会不被相应，此时，server 通过 /proc/sys/net/ipv4/tcp_abort_on_overflow 来决定如何返回，0 表示直接丢丢弃该 ACK，1 表示发送 RST 通知 client；相应的，client 则会分别返回 read timeout 或者 connection reset by peer。上面说的只是些理论，如果服务器不及时的调用 accept()，当 queue 满了之后，服务器并不会按照理论所述，不再对 SYN 进行应答，返回 ETIMEDOUT。根据这篇文档的描述，实际情况并非如此，服务器会随机的忽略收到的 SYN，建立起来的连接数可以无限的增加，只不过客户端会遇到延时以及超时的情况。

整个 TCP stack 有如下的两个 queue:
1. 一个是 half open(syn queue) queue(max(tcp_max_syn_backlog, 64))，用来保存 SYN_SENT 以及 SYN_RECV 的信息。
2. 另外一个是 accept queue(min(somaxconn, backlog))，保存 ESTAB 的状态，但是调用 accept()。

LISTEN 状态: Recv-Q 表示的当前等待服务端调用 accept 完成三次握手的 listen backlog 数值，也就是说，当客户端通过 connect() 去连接正在 listen() 的服务端时，这些连接会一直处于这个 queue 里面直到被服务端 accept()；Send-Q 表示的则是最大的 listen backlog 数值，这就就是上面提到的 min(backlog, somaxconn) 的值。
其余状态: 非 LISTEN 状态之前理解的没有问题。Recv-Q 表示 receive queue 中的 bytes 数量；Send-Q 表示 send queue 中的 bytes 数值。

==============================================================================

拥塞控制：慢启动，拥塞避免，快重传，快恢复
拥塞控制就是防止过多的数据注入网络中，这样可以使网络中的路由器或链路不致过载。
拥塞控制是一个全局性的过程，和流量控制不同，流量控制指点对点通信量的控制。
TCP拥塞控制算法
该算法包括三个主要部分：（1）加性增、乘性减 （2）慢启动 （3）对超时事件做出反应
1.加性增、乘性减
加性增: 如没有检测到丢包事件，每个RTT时间拥塞窗口值增加一个MSS (最大报文段长度)。
乘性减: 丢包事件后，拥塞窗口值减半。
　　CongWin值重复地经历一种升降循环，即重复的线性增长，然后又突然降至当前值懂得一半（当丢包事件发生时），这种循环，使得长寿命TCP连接的CongWin变化呈锯齿形状

2.慢启动:
　　在连接开始时, 拥塞窗口值 = 1 MSS（例如: MSS= 500 bytes & RTT = 200 msec，则初始化速率 = 20 kbps）。但可获得带宽可能 >> MSS/RTT，为了尽快达到期待的速率，我们可以在连接开始的时候，以指数级的速率增加，直到第一个丢失事件发生。
当连接开始的时候，速率呈指数式上升，直到第1次报文丢失事件发生为止:（1）每经过一个RTT，倍增拥塞窗口值；（2）每收到ACK，增加拥塞窗口。
总结: 初始速率很低，但以指数级的速率增加

3.对超时事件作出反应
实际中TCP对因超时而检测到的丢包事件作出的反应与对因收到3个冗余ACK而检测到的丢包事件做出的反应是不同的。
收到3个冗余ACK后：CongWin减半、窗口再线性增加。
检测超时事件后：CongWin值设置为1MSS、窗口再指数增长、到达一个阈值(Threshold，初始化时被设置为一个很大的值，以使它没有初始效应。每发生一个丢包事件，Threshold就会被设置为当前CongWin值的一半)后，再线性增长。
原因：3个冗余ACK指示网络还具有某些传送报文段的能力；3个冗余ACK以前的超时,则更为 “严重”。

当CongWin < Threshold时，发送者处于慢启动阶段, CongWin指数增长。
当CongWin > Threshold时，发送者处于拥塞避免阶段, CongWin线性增长。
当出现3个冗余确认时, 阈值Threshold设置为CongWin/2，且CongWin设置为Threshold。
当超时发生时，阈值Threshold设置为CongWin/2，并且CongWin设置为1 MSS。

==============================================================================

超时重传指的是，发送数据包在一定的时间周期内没有收到相应的ACK，等待一定的时间，超时之后就认为这个数据包丢失，就会重新发送。这个等待时间被称为RTO.  

每一次开始发送一个TCP segment的时候，就启动重传定时器，定时器的时间一开始是一个预设的值（Linux 规定为1s），随着通讯的变化以及时间的推移，这个定时器的溢出值是不断的在变化的,如果在ACK收到之前，定时器到期，协议栈就会认为这个片段被丢失，重新传送数据。
TCP在实现重传机制的时候，需要保证能够在同一时刻有效的处理多个没有被确认的ACK，也保证在合适的时候对每一个片段进行重传，有这样几点原则：

1这些被发送的片段放在一个窗口中，等待被确认，没有确认不会从窗口中移走，定时器在重传时间到期内，每个片段的位置不变
2只有等到ACK收到的时候，变成发送并ACK的片段，才会被从窗口中移走。
3如果定时器到期没有收到对应ACK， 就重传这个TCP segment

重传之后也没有办法完全保证，数据段一定被收到，所以仍然会重置定时器，等待ACK，如果定时器到期还是没有收到ACK，继续重传，这个过程重传的TCP segment一直留着队列之内。

超时重传的例子：
1. Server 发送80个字节 Part1，seq = 1 
2. Server 发送120个字节Part2，Seq = 81
3. Server发送160个字节Part3，Seq = 201，此包由于其他原因丢失
4. Client收到前2个报文段，并发送ACK = 201
5. Server发送140个字节Part4， Seq = 361
7. Server收到Client对于前两个报文段的ACK，将2个报文从窗口中移除，窗口有200个字节的余量
8. 报文3的重传定时器到期，没有收到ACK，进行重传
9. 这个时候Client已经收到报文4，存放在缓冲区中，也不会发送ACK【累计通知，发送ACK就表示3也收到了】，等待报文3，报文3收到之后，一块对3,4进行确认
10. Server收到确认之后，将报文3,4移除窗口，所有数据发送完成
这种方式会面临一个问题：客户端在等待报文3的时候，服务器如何处理报文4， 客户端这个期间内并没有发送任何报文，服务器并不知道报文3和报文4的状态，报文4可能会丢失，也可能会被客户端接收，那么如果超时了，我到底值该发送报文3 ，还是报文3和报文4 呢？
总结起来就是2中处理
1. 定时器溢出，重传3
2. 定时器溢出，重传3,4
对于怎么传的问题，在RFC2018中已经提供了一种方案: SACK,    详细可参考文章：TCP-IP详解：SACK选项（Selective Acknowledgment）
对于重传时间是如何计算的问题，在RFC2988中也提供了一种至今Linux使用的方案，详细介绍可以参考文章：TCP-IP详解 RTT and RTO

==============================================================================

快速重传机制
在超时重传中，重点是定时器溢出超时了才认为发送的数据包丢失，
快速重传机制，实现了另外的一种丢包评定标准，即如果我连续收到3次dup ACK，发送方就认为这个seq的包丢失了，立刻进行重传，这样如果接收端回复及时的话，基本就是在重传定时器到期之前，提高了重传的效率。

在传输过程中会出现out-of-order的现象，但是在滑动窗口中会有严格的顺序控制，假设有4，5，6三个待接收的数据包，先收到了5,6，协议栈是不会回复对5,6包的确认，而是根据TCP协议的规定，当接收方收到乱序片段时，需要重复发送ACK, 在这个地方会发送报文4 seq的ACK，表明需要报文4没有被接收到，如果此后收到的是报文7，那么仍然要回报文4 seq的ACK，如果连续发送3个 dup ACK，接收端认为这个片段已经丢失，进行快速重传。

==============================================================================

快速恢复算法
TCP Reno这个算法定义在RFC5681。快速重传和快速恢复算法一般同时使用。快速恢复算法是认为，你还有3个Duplicated Acks说明网络也不那么糟糕，所以没有必要像RTO超时那么强烈，并不需要重新回到慢启动进行，这样可能降低效率。所以协议栈会做如下工作

cwnd = cwnd/2 
sshthresh = cwnd 
然后启动快速恢复算法：
设置cwnd = ssthresh＋ACK个数＊MSS（一般情况下会是3个dup ACK）
重传丢失的数据包（对于重传丢失的那个数据包，可以参考TCP-IP详解：SACK选项）
如果只收到Dup ACK，那么cwnd = cwnd + 1， 并且在允许的条件下发送一个报文段
如果收到新的ACK, 设置cwnd = ssthresh， 进入拥塞避免阶段

其实TCP Reno算法就是在慢启动和拥塞避免的基础上增加了快速重传和快速恢复算法，避免了在拥塞不严重的状况下，过大的减小拥塞窗口，降低TCP的传输效率,

==============================================================================
https://www.zhihu.com/question/32255109
滑动窗口 -- 表征发送端和接收端的接收能力
拥塞窗口-- 表征中间设备的传输能力
TCP流量控制不是为了减少网络压力，那是TCP拥塞控制的作用。
TCP流量控制解决发送端与接收方吞吐量不匹配的问题，比如当一个发送端A每秒发10个数据包，而接收端B每秒只能接受1个数据包，那么就会出现丢包的情况，所以发送端与接收端要的吞吐量要匹配。

滑动窗口是接受数据端使用的窗口大小，用来告知发送端接收端的缓存大小，以此可以控制发送端发送数据的大小，从而达到流量控制的目的。
那么对于数据的发送端就是拥塞窗口了，拥塞窗口不代表缓存，拥塞窗口指某一源端数据流在一个RTT内可以最多发送的数据包数

流量控制：滑动窗口
TCP滑动窗口分为接受窗口，发送窗口
滑动窗口协议是传输层进行流控的一种措施，接收方通过通告发送方自己的窗口大小，从而控制发送方的发送速度，从而达到防止发送方发送速度过快而导致自己被淹没的目的。

对ACK的再认识，ack通常被理解为收到数据后给出的一个确认ACK，ACK包含两个非常重要的信息：一是期望接收到的下一字节的序号n，该n代表接收方已经接收到了前n-1字节数据，此时如果接收方收到第n+1字节数据而不是第n字节数据，接收方是不会发送序号为n+2的ACK的。
举个例子，假如接收端收到1-1024字节，它会发送一个确认号为1025的ACK,但是接下来收到的是2049-3072，它是不会发送确认号为3072的ACK,而依旧发送1025的ACK。

二是当前的窗口大小m，如此发送方在接收到ACK包含的这两个数据后就可以计算出还可以发送多少字节的数据给对方，假定当前发送方已发送到第x字节，则可以发送的字节数就是y=m-(x-n).这就是滑动窗口控制流量的基本原理

TCP流量控制需要的几个变量：
LastByteSent ：最后发送的数据包的序号，由发送端进行维护
LastByteACK：最后收到的ACK包确认的数据包序号，由发送端进行维护
rwnd：由发送方进行维护，表示接收方还有多少空余空间，由接收方发回（与ACK一起？待查）
LastByteRead：由接收方维护，表示应用程序最后从缓存里面读取的数据包的序号
RcvBuffer:接收方缓存的最大值

对于TCP会话的发送方，任何时候在其发送缓存内的数据都可以分为4类，“已经发送并得到对端ACK的”，“已经发送但还未收到对端ACK的”，“未发送但对端允许发送的”，“未发送且对端不允许发送”。“已经发送但还未收到对端ACK的”和“未发送但对端允许发送的”这两部分数据称之为发送窗口。

滑动窗口实现面向流的可靠性1）最基本的传输可靠性来源于“确认重传”机制。2）TCP的滑动窗口的可靠性也是建立在“确认重传”基础上的。3）发送窗口只有收到对端对于本段发送窗口内字节的ACK确认，才会移动发送窗口的左边界。4）接收窗口只有在前面所有的段都确认的情况下才会移动左边界。当在前面还有字节未接收但收到后面字节的情况下，窗口不会移动，并不对后续字节确认。以此确保对端会对这些数据重传。

==============================================================================
TCP连接上发送的字节序列在某一个瞬间分成了3个FIFO序列。
1.sendQ：在发送端底层实现中缓存的字节，这些字节已经写入网络流，还没有被接收端收到。
2.RecvQ：在接收端底层实现中缓存的字节，等待分配到应用程序——即从输入流中读取数据。
3.Delivered：接收着从输入流中已经读取到的字节。

sendQ中的字节既然存在就表明 对方还没有收到，是为了防止网络异常重发。out.write()是向sendQ追加字节。然后又sendQ向RecvQ发送的过程不能由用户程序看到和观察，并且以块（chunk）的形式传输，chunk的大小与write（）写入的字节大小无关。

从RecvQ读取数据时，字节从RecvQ发送到delivered中 ，转移的chunk的大小与RecVQ中的字节和read（BUF）中缓冲区的大小有关。

==============================================================================

SendQ和RecvQ缓冲队列，这两个缓冲区的容量在具体实现时会受一定的限制，虽然它们使用的实际内存大小会动态地增长和收缩，但还是需要一个硬性的限制，以防止行为异常的程序所控制的单一TCP连接将系统的内存全部消耗。正式由于缓冲区的容量有限，它们可能会被填满，事实也正是如此，如果与TCP的流量控制机制结合使用，则可能导致一种形式的死锁。

一旦RecvQ已满，TCP流控制机制就会产生作用（使用流控制机制的目的是为了保证发送者不会传输太多数据，从而超出了接收系统的处理能力），它将阻止传输发送端主机的SendQ中的任何数据，直到接收者调用输入流的read（）方法将RecvQ中的数据移除一部分到Delivered中，从而腾出了空间。发送端可以持续地写出数据，直到SendQ队列被填满，如果SendQ队列已满时调用输出流的write（）方法，则会阻塞等待，直到有一些字节被传输到RecvQ队列中，如果此时RecvQ队列也被填满了，所有的操作都将停止，直到接收端调用了输入流的read（）方法将一些字节传输到了Delivered队列中。

引出问题
我们假设SendQ队列和RecvQ队列的大小分别为SQS和RQS。将一个大小为n的字节数组传递给发送端write（）方法调用，其中n > SQS，直到有至少n-SQS字节的数据传递到接收端主机的RecvQ队列后，该方法才返回。如果n的大小超过了SQS+RQS，write（）方法将在接收端从输入流读取了至少n-(SQS+RQS)字节后才会返回。如果接收端没有调用read（）方法，大数据量的发送是无法成功的。特别是连接的两端同时分别调用它的输出流的write（）方法，而他们的缓冲区大小又大于SQS+RQS时，将会发生死锁：两个write操作都不能完成，两个程序都将永远保持阻塞状态。

下面考虑一个具体的例子，即主机A上的程序和主机B上的程序之间的TCP连接。假设A和B上的SQS和RQS都是500字节，下图展示了两个程序试图同时发送1500字节时的情况。主机A上的程序中的前500字节已经传输到另一端，另外500字节已经复制到了主机A的SendQ队列中，余下的500字节则无法发送，write（）方法将无法返回，直到主机B上程序的RecvQ队列有空间空出来，然而不幸的是B上的程序也遇到了同样的情况，而二者都没有及时调用read（）方法从自己的RecvQ队列中读取数据到Delivered队列中。因此，两个程序的write（）方法调用都永远无法返回，产生死锁。因此，在写程序时，要仔细设计协议，以避免在两个方向上传输大量数据时产生死锁。

==============================================================================
IP分片重组

==============================================================================

MTU最大传输单元，这个最大传输单元实际上和链路层协议有着密切的关系，EthernetII帧的结构DMAC+SMAC+Type+Data+CRC由于以太网传输电气方面的限制，每个以太网帧都有最小的大小64bytes最大不能超过1518bytes，对于小于或者大于这个限制的以太网帧我们都可以视之为错误的数据帧，一般的以太网转发设备会丢弃这些数据帧。

由于以太网EthernetII最大的数据帧是1518Bytes这样，刨去以太网帧的帧头（DMAC目的MAC地址48bit=6Bytes+SMAC源MAC地址48bit=6Bytes+Type域2bytes）14Bytes和帧尾CRC校验部分4Bytes那么剩下承载上层协议的地方也就是Data域最大就只能有1500Bytes这个值我们就把它称之为MTU。 
==============================================================================
tun/tap设备

==============================================================================
常见模型 reactor
异步rpc框架
socket写阻塞如何追查

==============================================================================
IP头：
版本号 4bit 0100(ipv4) 0110(ipv6)
首部长度 4bit 4字节为单位最小为5，最大为15
服务类型 8bit
数据报长度 16bit
ID号    16bit
分段标志 3bit
分段偏移 13bit
生存期TTL 8bit
协议号   8bit ICMP=1 TCP=16 UDP=7 OSPF=89
校验和   16bit
源IP    32bit
目的IP  32bit

TCP头：
源端口 16bit
目的端口 16bit
seq num 32bit
ack num 32bit
首部长度  4bit
保留位    6bit
TCP标志位 6bit
窗口大小  16bit
校验和    16bit
紧急指针  16bit
选项字段
数据字段
TCP头部最小20字节，最大60字节
TCP头部中没有包长度字段，因为TCP是流协议，没有流量边界
TCP有自己的流量拥塞控制算法，且依靠IP层分片

ARP头：
目的mac地址 6字节
源mac地址  6字节
类型      2字节 0x0800 IP报文 / 0x0806 ARP报文 / 0x8035 RARP报文
校验和     4字节
以太网最小帧长64字节

UDP头：
源端口 16bit
目的端口 16bit
用户数据报长度 16bit
校验和 16bit
数据字段
UDP头最小8字节

==============================================================================
FTP协议：
端口20 传送文件数据
端口21 传送控制指令，如连接请求等

SNMP协议：
端口161 管理进程获取SNMP代理数据
端口162 SNMP代理主动向管理进程发送数据

DNS协议：
TCP协议 端口53 DNS区域文件传送
UDP协议 端口53 域名解析服务

RIP协议：距离矢量协议
UDP 端口520 RIPv1使用广播进行邻居 RIPv2使用组播224.0.0.9进行路由表更新

OSPF协议：
协议号89, 全部路由器 224.0.0.5 DR路由器 224.0.0.6

BGP协议：
TCP 端口号179

